{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachaudhry/FastAI-22-23/blob/main/course_part_2/01_Introduction_to_Generative_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWf8IkOxmHvR"
      },
      "source": [
        "# A Hands on Intro to Generative Modeling Using HF Diffusers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhOfOFa-hj2-"
      },
      "source": [
        "_Github's renderer tends to break with these output heavy notebooks. So, the versions saved here will have all outputs cleared._\n",
        "\n",
        "_In case I decide to retain outputs, then visit the [NB Viewer](https://nbviewer.org/github/bachaudhry/FastAI-22-23/blob/main/course_part_2/01_Introduction_to_Generative_Modeling.ipynb) link for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWN-jE3PMpBD",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -Uq diffusers transformers fastcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAa1iL_SmSyN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from fastcore.all import concat\n",
        "from huggingface_hub import notebook_login\n",
        "from PIL import Image\n",
        "\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "torch.manual_seed(44)\n",
        "if not (Path.home()/'.cache/huggingface' / 'token').exists(): notebook_login()\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdcLBW9jpd_M"
      },
      "source": [
        "## Setting Up the Stable Diffusion Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOVz0CClmS2e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",\n",
        "                                               variant=\"fp16\",\n",
        "                                               torch_dtype=torch.float16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N0H1Kw_mS6r"
      },
      "outputs": [],
      "source": [
        "# Checking location of the model weights\n",
        "!ls ~/.cache/huggingface/hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yo4uqtBmS-t"
      },
      "outputs": [],
      "source": [
        "# In case the GPU has insufficient memory\n",
        "# pipe.enable_attention_slicing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a74ajQcTmTCr"
      },
      "outputs": [],
      "source": [
        "# Testing first prompt\n",
        "prompt = \"A picture of polar bear in the style of national geographic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Gw5pBxmTG3"
      },
      "outputs": [],
      "source": [
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6OLZXh3mTKl"
      },
      "outputs": [],
      "source": [
        "# Using different seed values\n",
        "torch.manual_seed(8161)\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LCVTbK9MacU"
      },
      "outputs": [],
      "source": [
        "# Using different seed values\n",
        "torch.manual_seed(42)\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gKTSWLHhzZR"
      },
      "source": [
        "As diffusion models generate images from random noise after a series of steps, we can play around with the number of steps to see the effects on the model's outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfu1-nawmTOo"
      },
      "outputs": [],
      "source": [
        "# Taking the manual  seed setting from the last cell\n",
        "torch.manual_seed(42)\n",
        "pipe(prompt, num_inference_steps=3).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35pis0e6mTS2"
      },
      "outputs": [],
      "source": [
        "# Increase the number of steps to 10\n",
        "torch.manual_seed(42)\n",
        "pipe(prompt, num_inference_steps=10).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiR-df1xmTXi"
      },
      "outputs": [],
      "source": [
        "# Increase the number of steps to 16\n",
        "torch.manual_seed(42)\n",
        "pipe(prompt, num_inference_steps=16).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkj9P1cTik7x"
      },
      "outputs": [],
      "source": [
        "# Let's take it up to 40\n",
        "torch.manual_seed(42)\n",
        "pipe(prompt, num_inference_steps=40).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWa96ZQitmN"
      },
      "outputs": [],
      "source": [
        "# Cranking to 100\n",
        "torch.manual_seed(42)\n",
        "pipe(prompt, num_inference_steps=100).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5mf4JUCjMlK"
      },
      "source": [
        "## Classifier Free Guidance\n",
        "\n",
        "This method is used to increase adherence of the outputs to the conditioning signal used in the prompts.\n",
        "\n",
        "Larger guidance settings increase adherence at the expense of diversity. The default setting is `7.5`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eRBPapRi_VP"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "  w, h = imgs[0].size\n",
        "  grid = Image.new('RGB', size=(cols * w, rows * h))\n",
        "  for i, img in enumerate(imgs):\n",
        "    grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "  return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNP_c3nojtb9"
      },
      "outputs": [],
      "source": [
        "# Testing guidance parameter settings\n",
        "num_rows, num_cols = 4, 4\n",
        "prompts = [prompt] * num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7DD1qFOkMr4"
      },
      "outputs": [],
      "source": [
        "images = concat(pipe(prompts, guidance_scale=g).images for g in [1.1, 4, 10, 20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuDQSlUKkPOt"
      },
      "outputs": [],
      "source": [
        "image_grid(images, rows=num_rows, cols=num_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teDbi3_7lJyr"
      },
      "source": [
        "## Negative Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zNG9NRWkPVA"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(64)\n",
        "prompt = \"Early morning in the Himalayas\"\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBmfMA_3kPbd"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(64)\n",
        "pipe(prompt, negative_prompt=\"red\").images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-Bzln67kPh7"
      },
      "outputs": [],
      "source": [
        "# Testing different guidance scales like we did in the previous section\n",
        "torch.manual_seed(64)\n",
        "\n",
        "num_rows, num_cols = 4, 4\n",
        "prompts = [prompt] * num_cols\n",
        "neg_prompt = ['blue'] *num_cols\n",
        "\n",
        "imgs = concat(pipe(prompts, negative_prompt=neg_prompt, guidance_scale=g).images for g in [1.1, 4, 10, 20])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_grid(imgs, rows=num_rows, cols=num_cols)"
      ],
      "metadata": {
        "id": "94ZNNeIRUvtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SibaK6XrpCHR"
      },
      "source": [
        "## Image to Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Iu4gWxPeO2GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recovering GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XuA2MC_KO6Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "462z1BbgmbLY"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from fastdownload import FastDownload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4niIBZbTpD3B"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    variant=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTdgt7hLpD8Q"
      },
      "outputs": [],
      "source": [
        "# Using the lesson example\n",
        "p = FastDownload().download('https://cdn-uploads.huggingface.co/production/uploads/1664665907257-noauth.png')\n",
        "init_image = Image.open(p).convert(\"RGB\")\n",
        "init_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Oz0hHexqEKm"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(21)\n",
        "prompt = \"Owl on a moonlit night, photorealistic 4K\"\n",
        "images = pipe(prompt=prompt, num_images_per_prompt=3,\n",
        "              image=init_image, strength=0.8, num_inference_steps=50).images\n",
        "image_grid(images, rows=1, cols=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J-rFYuJpEAE"
      },
      "outputs": [],
      "source": [
        "# Selecting a generated image to seed the series of prompts\n",
        "init_image=images[1]\n",
        "\n",
        "torch.manual_seed(21)\n",
        "prompt = \"An Owl in the style of Animal Planet.\"\n",
        "images = pipe(prompt=prompt, num_images_per_prompt=3,\n",
        "              image=init_image, strength=1, num_inference_steps=100).images\n",
        "image_grid(images, rows=1, cols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAOmYE5BySmu"
      },
      "source": [
        "## Fine Tuning\n",
        "\n",
        "This [blogpost](https:/https://lambdalabs.com/blog/how-to-fine-tune-stable-diffusion-how-we-made-the-text-to-pokemon-model-at-lambda//) shows how the folks at Lambda Labs applied fine tuning to get a text-to-pokemon model.\n",
        "\n",
        "**LINK to NB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLPk-pOhzxQH"
      },
      "source": [
        "## Textual Inversion\n",
        "\n",
        "Using this technique, we can \"teach\" a new word to the text model and train its embeddings accordingly.\n",
        "\n",
        "The token vocabulary is updated, while the model weights are frozen - apart from the text encoder - and the generator is trained using a sample of representative images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsQlVaH3zzsI"
      },
      "outputs": [],
      "source": [
        "# Using the embeddings from the above link\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",\n",
        "                                               variant=\"fp16\",\n",
        "                                               torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For textual inversion, we will be using HuggingFace's **[sd-concepts-library/tim-sale](https://https://huggingface.co/sd-concepts-library/tim-sale)** style."
      ],
      "metadata": {
        "id": "XfgdwcuymsmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjo87Mtxz9dz"
      },
      "outputs": [],
      "source": [
        "embeds_url = \"https://huggingface.co/sd-concepts-library/tim-sale/blob/main/learned_embeds.bin\"\n",
        "embeds_path = FastDownload().download(embeds_url)\n",
        "#embeds_dict = torch.load(str(embeds_path), map_location=torch.device(\"cpu\"))  # Throws an unpickling error\n",
        "embeds_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "embeds_new_path = '/content/drive/MyDrive/Learned_Embs/learned_embeds(1).bin'\n",
        "embeds_dict =  torch.load(str(embeds_new_path), map_location=torch.device(\"cpu\"))"
      ],
      "metadata": {
        "id": "YiHc6hXhZoHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.tokenizer\n",
        "text_encoder = pipe.text_encoder\n",
        "new_token, embeds = next(iter(embeds_dict.items()))\n",
        "embeds = embeds.to(text_encoder.dtype)\n",
        "new_token"
      ],
      "metadata": {
        "id": "ysS0GvrORjet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This new token will be added to the tokenizer and the embeddings to the embeddings table\n",
        "assert tokenizer.add_tokens(new_token) == 1, \"This token already exists!\""
      ],
      "metadata": {
        "id": "t8be1e8-RtqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
        "\n",
        "text_encoder.get_input_embeddings().weight.data[new_token_id] = embeds"
      ],
      "metadata": {
        "id": "UAzRAyClRtzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running inference and referring to the newly added style\n",
        "torch.manual_seed(1000)\n",
        "prompt = \"Supergirl smiling in the style of <cat-toy>\"\n",
        "images = pipe(prompt, num_images_per_prompt=4, num_inference_steps=100).images\n",
        "image_grid(images, 1, 4)"
      ],
      "metadata": {
        "id": "h-yCiXusRt7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(876)\n",
        "prompt = \"The Joker in the style of <cat-toy>\"\n",
        "images = pipe(prompt, num_images_per_prompt=4, guidance_scale=7.5, num_inference_steps=100).images\n",
        "image_grid(images, 1, 4)"
      ],
      "metadata": {
        "id": "ZINHv7w4o8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(76)\n",
        "prompt = \"Athlete running in the style of <cat-toy>\"\n",
        "images = pipe(prompt, num_images_per_prompt=4, num_inference_steps=100).images\n",
        "image_grid(images, 1, 4)"
      ],
      "metadata": {
        "id": "_u_jo32hRuD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(78)\n",
        "prompt = \"Close up of Batman in the style of <cat-toy>\"\n",
        "images = pipe(prompt, num_images_per_prompt=4, num_inference_steps=100).images\n",
        "image_grid(images, 1, 4)"
      ],
      "metadata": {
        "id": "G36ozH_creV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dreambooth"
      ],
      "metadata": {
        "id": "cU4pAPWmov3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique of fine tuning is used to introduce new subjects by providing a few images as examples.\n",
        "\n",
        "The difference from Textual Inversion is that we select an existing token in the vocab and fine-tune the model to bring the token close to the images that were provided."
      ],
      "metadata": {
        "id": "i1j5np9NrzUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recovering GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5hyDhiFEvxPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll stick with JH's example from the lesson\n",
        "# Using the rare sks token token to qualify the term person\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"pcuenq/jh_dreambooth_1000\",\n",
        "                                               torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "uLYYVyoBRuJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(44)\n",
        "\n",
        "prompt = \"Painting of sks person in the style of Paul Signac\"\n",
        "images = pipe(prompt, num_images_per_prompt=4).images\n",
        "image_grid(images, 1, 4)"
      ],
      "metadata": {
        "id": "T0MRDLrwozZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latents and Callbacks"
      ],
      "metadata": {
        "id": "CTNJrZIqyMzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Diffusion has a major downside i.e. the reverse denoising process is slow and the models are known to consume alot of memory since they operate in pixel space.\n",
        "\n",
        "Instead of using the actual pixel space, we can apply the diffusion process over a lower resolution latent space. This is the key difference between standard and latent diffusion models - where Stable Diffusion belongs to the latter class of models.\n",
        "\n"
      ],
      "metadata": {
        "id": "vZVfch8KyRAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The stable diffusion pipeline can send intermediate latents to a callback function.\n",
        "# Running these latents through an image decoder i.e. VAE component, we can observe the\n",
        "# denoising process.\n",
        "vae = pipe.vae\n",
        "images = []\n",
        "\n",
        "def latents_callback(i, t, latents):\n",
        "  latents = 1 / 0.18215 * latents\n",
        "  image = vae.decode(latents).sample[0]\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.cpu().permute(1, 2, 0).numpy()\n",
        "  images.extend(pipe.numpy_to_pil(image))\n",
        "\n",
        "prompt = \"Oil on canvas portrait of Gandhi reading a book.\"\n",
        "torch.manual_seed(1452)\n",
        "\n",
        "final_image = pipe(prompt, callback=latents_callback, callback_steps=8).images[0]\n",
        "images.append(final_image)\n",
        "image_grid(images, rows=1, cols=len(images))"
      ],
      "metadata": {
        "id": "8bsiMOV4ozhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a closer look at the pipeline. We'll first get rid of the existing pipe object."
      ],
      "metadata": {
        "id": "kFuzDA3k01lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe"
      ],
      "metadata": {
        "id": "bt1jXVclozx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recreating the Pipeline From Scratch"
      ],
      "metadata": {
        "id": "rW6c-FQP1LNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recovering GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "NX33_eSwIAd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer"
      ],
      "metadata": {
        "id": "5VbltqBCoz5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "id": "3pTL77T8RuN_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the VAE and the UNET\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "# The VAE below has been fine tuned for more steps\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "id": "aiBFTChcRsU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lesson notebook uses a different scheduler, called the K-LMS Scheduler. Also, we will need to use the same noising schedule that was used during training."
      ],
      "metadata": {
        "id": "x_OcIQ1C3De_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The K-LMS scheduler evolves betas over 1000 steps as follows\n",
        "beta_start, beta_end = 0.00085, 0.012\n",
        "plt.plot(torch.linspace(beta_start**0.5, beta_end**0.5, 1000) ** 2)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('Î²');"
      ],
      "metadata": {
        "id": "Y6UGFrIDRsfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\",\n",
        "                                 num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "1cATYp9y3A1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameters to be used for generation\n",
        "\n",
        "prompt = [\"a photograph of a scuba diver riding a bicycle\"]\n",
        "\n",
        "height = 512\n",
        "width = 512\n",
        "num_inference_steps = 75\n",
        "guidance_scale = 7.5\n",
        "batch_size = 1"
      ],
      "metadata": {
        "id": "QiVohdOC3BMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the prompt\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length,\n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "text_input['input_ids']"
      ],
      "metadata": {
        "id": "dYj1wSWG3BZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where this is the padding token\n",
        "tokenizer.decode(49407)"
      ],
      "metadata": {
        "id": "SmFCO0C93CMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what the attention mask looks like\n",
        "text_input['attention_mask']"
      ],
      "metadata": {
        "id": "4DiUmCqw3Caj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The text encoder provides embeddings for  our prompt\n",
        "text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\n",
        "text_embeddings.shape"
      ],
      "metadata": {
        "id": "z7l9zOurRJE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the embeddings required to perform unconditional generation. The empty string\n",
        "# is created to handle this and it allows the model to run wild with its generations.\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "uncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\n",
        "uncond_embeddings.shape"
      ],
      "metadata": {
        "id": "wxqcRENN5-co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier free guidance requires two forward passes. One with conditioned input\n",
        "# and the other with unconditional embeddings and both are concatenated into a single batch\n",
        "# to avoid two forward passes.\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Denoising starts from pure Gaussian noise, which become our initial latents\n",
        "torch.manual_seed(100)\n",
        "latents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\n",
        "latents = latents.to(\"cuda\").half()\n",
        "latents.shape"
      ],
      "metadata": {
        "id": "YF0YoG5S5-3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "# Scaling the initial noise by the standard deviation required by the scheduler\n",
        "latents = latents * scheduler.init_noise_sigma"
      ],
      "metadata": {
        "id": "17CIaGae5_Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what the outputs of the above look like\n",
        "scheduler.timesteps"
      ],
      "metadata": {
        "id": "OBO81g-h5_i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#...and\n",
        "scheduler.sigmas"
      ],
      "metadata": {
        "id": "cuOwcwsb5_xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(scheduler.timesteps, scheduler.sigmas[:-1]);"
      ],
      "metadata": {
        "id": "3R3CpTYp-FDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Denoising Loop\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
        "  input = torch.cat([latents] * 2)\n",
        "  input = scheduler.scale_model_input(input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # Perform guidance\n",
        "  pred_uncond, pred_text = pred.chunk(2)\n",
        "  pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample\n",
        "  latents = scheduler.step(pred, t, latents).prev_sample\n"
      ],
      "metadata": {
        "id": "63unWwMI-Fiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our latents now contain the denoised representation of the image.\n",
        "# The VAE decoder converts it back to pixel space.\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(1 / 0.18215 * latents).sample"
      ],
      "metadata": {
        "id": "M7HTBAKn-N0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the image to PIL\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "image = (image * 255).round().astype(\"uint8\")\n",
        "Image.fromarray(image)"
      ],
      "metadata": {
        "id": "kR47uC0z-OMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting The Above Into Functions"
      ],
      "metadata": {
        "id": "oj_8mkWrR5wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"a photograph of a scuba diver riding a bicycle\",\n",
        "    \"a pastel painting of a scuba diver riding a bicycle in the style of Degas \"\n",
        "]"
      ],
      "metadata": {
        "id": "RTk2HB2p-F9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_enc(prompts, max_len=None):\n",
        "  if max_len is None: max_len = tokenizer.model_max_length\n",
        "  inp = tokenizer(prompts, padding=\"max_length\", max_length=max_len, truncation=True,\n",
        "                  return_tensors=\"pt\")\n",
        "  return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
        "\n",
        "def mk_img(t):\n",
        "  image = (t / 2 + 0.5).clamp(0, 1).detach().cpu().permute(1, 2, 0).numpy()\n",
        "  return Image.fromarray((image * 255).round().astype(\"uint8\"))"
      ],
      "metadata": {
        "id": "qnTZgo7YRnoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mk_samples(prompts, g=11, seed=100, steps=75):\n",
        "  bs = len(prompts)\n",
        "  text = text_enc(prompts)\n",
        "  uncond = text_enc([\"\"] * bs, text.shape[1])\n",
        "  emb = torch.cat([uncond, text])\n",
        "  if seed: torch.manual_seed(seed)\n",
        "\n",
        "  latents = torch.randn((bs, unet.in_channels, height // 8, width // 8))\n",
        "  scheduler.set_timesteps(steps)\n",
        "  latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
        "\n",
        "  for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
        "    inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
        "    with torch.no_grad(): u, t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
        "    pred = u + g * (t - u)\n",
        "    latents = scheduler.step(pred, ts, latents).prev_sample\n",
        "\n",
        "  with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample"
      ],
      "metadata": {
        "id": "pE6Q4wZlRnyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = mk_samples(prompts)"
      ],
      "metadata": {
        "id": "NKvqTdBERn10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "for img in images: display(mk_img(img))"
      ],
      "metadata": {
        "id": "oDwggg4cRo5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vR3e79VuVfSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "mount_file_id": "1GFmo7hkmEooZUi2Mv_3HsFK2WtVEoyU4",
      "authorship_tag": "ABX9TyMVHCSrQswW6fw8zop/y+cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}