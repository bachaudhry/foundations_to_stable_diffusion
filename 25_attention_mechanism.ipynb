{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7589b1d9-4782-45fc-8ab8-3a554bf7d20f",
   "metadata": {},
   "source": [
    "# **Introducing The Attention Component of Transformers** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00eedd-d04c-4268-9cfc-ac0e47a0ceb6",
   "metadata": {},
   "source": [
    "According to Perplexity **Attention**, which made a huge splash in this [seminal paper](https://arxiv.org/abs/1706.03762), can briefly be defined as: \n",
    "\n",
    ">The attention mechanism mimics human cognitive processes by allowing a model to prioritize certain inputs over others based on their relevance to the task at hand. This is particularly useful in scenarios where the input data is large and complex, enabling the model to selectively concentrate on the most pertinent elements while ignoring less relevant information.\n",
    ">\n",
    "> ### Key Concepts\n",
    ">\n",
    ">    **Encoder-Decoder Architecture**: The attention mechanism is often employed within an encoder-decoder framework. The encoder processes the input sequence and generates a set of hidden states, while the decoder uses these states to produce the output sequence. Traditional models would pass only the final hidden state from the encoder to the decoder, but attention allows for all hidden states to be considered1\n",
    ">    \n",
    ">    **Attention Weights**: The mechanism assigns weights to different parts of the input, indicating their relative importance. These weights are dynamically calculated during model training and are used to create a context vector that emphasizes significant input elements.\n",
    ">\n",
    "> ### How Attention Works\n",
    ">\n",
    ">    **Calculating Attention Scores**: For each element in the output sequence, attention scores are computed by comparing it with all elements in the input sequence. This can be done using methods such as dot-product or additive attention, where each score reflects how relevant an input element is to a particular output element.\n",
    ">\n",
    ">    **Creating Context Vectors**: The attention scores are normalized using a softmax function to produce a probability distribution. This distribution is then used to compute a weighted sum of the input elements, resulting in a context vector that highlights important features.\n",
    ">\n",
    ">    **Decoding Process**: The context vector is fed into the decoder alongside its current hidden state. This allows the decoder to generate output tokens based on both the immediate context and relevant parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28663616-262e-4899-9f57-95c9c5cf914a",
   "metadata": {},
   "source": [
    "It is important to note that Stable Diffusion's implementation of Attention is quite sub-optimal. We may consider moving to better approaches in later NBs.\n",
    "\n",
    "Initially, we will focus on **1d-Attention**, which was predominantly used for NLP. For Stable Diffusion, we will flatten all pixel rows into single vectors for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bdce3b-469b-4a38-99d8-21b9bbbc9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "from torch import nn\n",
    "from miniai.activations import *\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers.models.attention import Attention # AttentionBlock has been deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2de7765-8619-4951-8312-45692088c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "# Creating a tensor to represent a 16x16 image, with 32 channels and a batch size of 64 (NCHW)\n",
    "# NLP implementations call HxW (16x16) a sequence. Sequence mostly preceeds dimension / channel\n",
    "x = torch.randn(64, 32, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60d375e-7bca-49cb-8d82-3856147caf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO replicate 1d-attention, we first need to flatten out the input tensor\n",
    "# in view(), -1 stands for 'everything else'. Transposing will give us NLP's equivalent of \n",
    "# NSD (BatchxChannelxDimension)\n",
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66656eb-8275-4c8b-9b0c-cae3ab1490e9",
   "metadata": {},
   "source": [
    ">Self-attention is a crucial mechanism in modern neural networks, especially within the context of natural language processing (NLP) and transformer architectures. It allows models to weigh the significance of different parts of an input sequence relative to each other, enabling the capture of complex dependencies and contextual relationships.\n",
    ">\n",
    "> ### Key Components of Self-Attention\n",
    ">\n",
    "> Self-attention operates using three main components derived from the input sequence:\n",
    ">\n",
    ">    **Query (Q)**: This vector represents the current focus or context for a specific word. It is generated through a linear transformation of the input embedding.\n",
    ">\n",
    ">    **Key (K)**: Each word in the input sequence has an associated key vector, which serves as a reference point. The model compares the query vector with all key vectors to determine relevance.\n",
    ">\n",
    ">    **Value (V)**: The value vectors hold the actual information content associated with each word. After calculating attention scores based on the similarity between queries and keys, these value vectors are weighted accordingly to produce the output.\n",
    ">\n",
    "> ### Process of Self-Attention\n",
    ">\n",
    ">    **Linear Transformations**: The input embeddings are transformed into three separate matrices—Q, K, and V—using learned weight matrices.\n",
    ">\n",
    ">    **Attention Scores Calculation**: The attention score for each pair of words is computed by taking the dot product of the query vector with all key vectors. This score indicates how much focus should be placed on each word when processing a particular word.\n",
    ">\n",
    ">    **Softmax Normalization**: The scores are normalized using a softmax function to create a probability distribution, ensuring that they sum to one.\n",
    ">\n",
    ">    **Weighted Sum**: The output for each word is obtained by calculating a weighted sum of the value vectors, where weights correspond to the normalized attention scores.\n",
    ">\n",
    ">    **Final Output**: The resulting context-aware representations are then passed through additional layers, typically including feed-forward neural networks, to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4e4b61-f600-4228-8ec9-e2ff1301a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input channels\n",
    "ni = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f2e566-4478-4f8d-83ae-74c7f8860c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need 3 projections for 32 in channels to 32 out channels\n",
    "# Creating simple linear layers (matmul plus a bias). Randomly initializing.\n",
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f86fd31-9994-412b-bdaf-88012707223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]),\n",
       " torch.Size([64, 256, 32]),\n",
       " torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For self attention, the technical parlance refers to these projections as keys, queries and values\n",
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)\n",
    "\n",
    "k.shape, q.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6257ed5-1648-45dc-beee-cbc41f2498f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matmul with the transpose. For every 64 items in the batch and for 256 pixels, we now have 256 weights\n",
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8de99-1357-427a-b024-7395c9cfbd98",
   "metadata": {},
   "source": [
    "Time to put the last few cell blocks into a `SelfAttention()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8efd48c-adf7-4283-9aac-5982166f4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a class for self attention. Note that this self-attention approach is more geared\n",
    "# towards resnets.\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm  = nn.GroupNorm(1, ni) # Basically, BatchNorm for sets of channels\n",
    "        self.q     = nn.Linear(ni, ni)\n",
    "        self.k     = nn.Linear(ni, ni)\n",
    "        self.v     = nn.Linear(ni, ni)\n",
    "        self.proj  = nn.Linear(ni, ni) # final projection to map items to different scales\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n, c, h, w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        # Matmul changes the scale of weights, normalizing to the original scale\n",
    "        s = (q@k.transpose(1, 2)) / self.scale\n",
    "        x = s.softmax(dim=-1) @ v\n",
    "        x = self.proj(x) # Secondary projection\n",
    "        x = x.transpose(1, 2).reshape(n,c,h,w) # reshaping back to the original\n",
    "        return x + inp # adding outputs to the original. Diffusers does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaca83fa-a690-413c-8926-0227e43bb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32) # self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95cb2d34-7a7f-4a2b-9817-f4324de5c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the self attention layer on the randomly generated numbers. Transpose ops above\n",
    "# ensure that the shape isn't changed\n",
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a1c2a9-cc4e-43fb-bd06-cadd57db6713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e84ad-1413-41bb-9278-872bf017a484",
   "metadata": {},
   "source": [
    "We need to be sure that our outputs align with Diffusers' `Attention` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31fc5808-64ea-4d9c-a2f4-a65864e04b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_params(a, b):\n",
    "    # Copy weights and biases from b to a\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf58ed9f-41dd-43c8-a731-4f2e8fbc7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffuser attention, updated since AttentionBlock ws deprecated\n",
    "at = Attention(32, dim_head=32, out_dim=32, norm_num_groups=1, residual_connection=1)\n",
    "# Comparing out q,k,v values to the ones from `at`\n",
    "src = sa.q, sa.k, sa.v, sa.proj, sa.norm\n",
    "dst = at.to_q, at.to_k, at.to_v, at.to_out[0], at.group_norm\n",
    "#  Pairwise zipping \n",
    "for s, d in zip(src, dst): cp_params(s, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e5156e-f227-4274-9dfd-1db3d6b0e878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = at(x)\n",
    "rb[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6efaf-3d6c-4b83-baf6-40aea3c20d86",
   "metadata": {},
   "source": [
    "The similarity of results means that our attention block is now similar to the diffusers attention block. Alternatively, we can also run the following code for our projection calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8dc59ef-32d6-4784-bbe5-34b7b38d1af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 96])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of three separate projections, we can create a single one. However the final dimension size will be larger\n",
    "# given the three sets of multiplications here.\n",
    "sqkv = nn.Linear(ni, ni*3)\n",
    "st = sqkv(t)\n",
    "st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db435d76-808a-465e-b6e4-4439fa77d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunking allows us to split along the last dimension to get q, k, v\n",
    "q, k, v = torch.chunk(st, 3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a350634-3b20-44ab-be91-20be1e896203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above, this is an alternate - and more concise - version of SelfAttention which has \n",
    "# a single projection for q,k,v.\n",
    "\n",
    "# This approach should also reduce computational overheads if we're working with standard PyTorch based frameworks.\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm  = nn.BatchNorm2d(ni)\n",
    "        self.qkv   = nn.Linear(ni, ni*3)\n",
    "        self.proj  = nn.Linear(ni, ni)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        n, c, h, w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        q, k, v = torch.chunk(self.qkv(x), 3, dim=-1) # Applying chunking to split along the last dim\n",
    "        s = (q@k.transpose(1, 2)) / self.scale\n",
    "        x = s.softmax(dim=-1) @ v\n",
    "        x = self.proj(x).transpose(1, 2).reshape(n, c, h, w)\n",
    "        return x + inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3a099a4-76d7-40ca-ba89-3ac30cdaf3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a984235-de66-44fc-afc4-620c86e13e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0094, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653b3d6-48f5-41c6-8188-b29e52037d94",
   "metadata": {},
   "source": [
    "Now that we've figured out how to apply self-attention, it is time to chuck it out the window **because this approach is never used in Stable Diffusion**.\n",
    "\n",
    "Instead, we will use **Multi-Headed Attention**. Based on Perplexity...\n",
    "\n",
    "> The choice of using multi-headed attention over self-attention in training stable diffusion models is primarily driven by the need for enhanced feature representation and effective integration of textual and visual information.\n",
    "> \n",
    "> ### Importance of Multi-Headed Attention\n",
    "> \n",
    "> #### 1. Diverse Representation:\n",
    "> \n",
    "> Multi-headed attention allows the model to capture different aspects of the input data simultaneously. Each attention head can focus on various parts of the input sequence, enabling the model to learn multiple relationships and features from the data. This diversity is crucial for tasks like text-to-image generation, where different textual prompts may relate to various visual features in an image.\n",
    "> \n",
    "> #### 2. Cross-Attention Mechanism:\n",
    "> \n",
    "> In stable diffusion models, cross-attention plays a vital role in merging information from text prompts with image features. The cross-attention layers enable the model to align and integrate the textual description with specific regions of the image, facilitating coherent image generation based on the provided prompt.\n",
    ">\n",
    "> This mechanism is essential for ensuring that generated images are consistent with their corresponding textual descriptions.\n",
    "> #### 3. Preservation of Spatial Details:\n",
    ">\n",
    "> While self-attention is useful for understanding relationships within a single input (like an image), it does not effectively manage the integration of external information (like text). In contrast, multi-headed cross-attention allows for the preservation of geometric and spatial details during transformation processes, which is critical in maintaining the integrity of the original image while incorporating new elements from the text.\n",
    ">\n",
    "> ### Limitations of Self-Attention in This Context\n",
    "> \n",
    "> Self-attention, while beneficial for capturing internal dependencies within an input sequence, does not provide the same level of flexibility when it comes to integrating external information. In stable diffusion models, relying solely on self-attention could lead to challenges in maintaining coherence between generated images and their corresponding textual prompts. The self-attention mechanism tends to focus more on preserving shape and structure rather than effectively merging different types of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f3a1b-18f6-4906-b9b1-0d80168ea6b1",
   "metadata": {},
   "source": [
    "It is important to note that softmax tends to highlight certain weights more than others due to the way it scales outputs. So, in the case of single-headed attention, it would pick a single pixel almost exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "328ea638-8e70-4c75-8a57-80e65c3395ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from the diffusers code and is the traditional way to approach the problem.\n",
    "def heads_to_batch(x, heads):\n",
    "    # batch, pixels, channels\n",
    "    n, sl, d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1) # reshaping so that we have 64 images x 256 pixels x 4 heads x 32/8 channels\n",
    "    return x.transpose(2, 1).reshape(n*heads, sl, -1) # nx4 -->reshape to combine\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n, sl, d = x.shape \n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1, sl, d*heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6f307-064b-4052-b5b8-d7145a28534d",
   "metadata": {},
   "source": [
    "`einops` allows us to use `rearrange` which is a cool rethinking of Einstein summation notation to enable tensor re-arranging operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ae1a065-87a5-4ff6-ad41-7bc48164b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c71cf00b-b09a-46c6-a951-46245b55a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\n",
       "This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\n",
       "stack, concatenate and other operations.\n",
       "\n",
       "Examples for rearrange operation:\n",
       "\n",
       "```python\n",
       "# suppose we have a set of 32 images in \"h w c\" format (height-width-channel)\n",
       ">>> images = [np.random.randn(30, 40, 3) for _ in range(32)]\n",
       "\n",
       "# stack along first (batch) axis, output is a single array\n",
       ">>> rearrange(images, 'b h w c -> b h w c').shape\n",
       "(32, 30, 40, 3)\n",
       "\n",
       "# concatenate images along height (vertical axis), 960 = 32 * 30\n",
       ">>> rearrange(images, 'b h w c -> (b h) w c').shape\n",
       "(960, 40, 3)\n",
       "\n",
       "# concatenated images along horizontal axis, 1280 = 32 * 40\n",
       ">>> rearrange(images, 'b h w c -> h (b w) c').shape\n",
       "(30, 1280, 3)\n",
       "\n",
       "# reordered axes to \"b c h w\" format for deep learning\n",
       ">>> rearrange(images, 'b h w c -> b c h w').shape\n",
       "(32, 3, 30, 40)\n",
       "\n",
       "# flattened each image into a vector, 3600 = 30 * 40 * 3\n",
       ">>> rearrange(images, 'b h w c -> b (c h w)').shape\n",
       "(32, 3600)\n",
       "\n",
       "# split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n",
       ">>> rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape\n",
       "(128, 15, 20, 3)\n",
       "\n",
       "# space-to-depth operation\n",
       ">>> rearrange(images, 'b (h h1) (w w1) c -> b h w (c h1 w1)', h1=2, w1=2).shape\n",
       "(32, 15, 20, 12)\n",
       "\n",
       "```\n",
       "\n",
       "When composing axes, C-order enumeration used (consecutive elements have different last axis)\n",
       "Find more examples in einops tutorial.\n",
       "\n",
       "Parameters:\n",
       "    tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).\n",
       "            list of tensors is also accepted, those should be of the same type and shape\n",
       "    pattern: string, rearrangement pattern\n",
       "    axes_lengths: any additional specifications for dimensions\n",
       "\n",
       "Returns:\n",
       "    tensor of the same type as input. If possible, a view to the original tensor is returned.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/miniai/lib/python3.11/site-packages/einops/einops.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rearrange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06501c1-de02-4b4a-b9ce-362fe433330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor rearrangement\n",
    "# Take our rank 3 tensor, containing the first dim of length n, the second dim of length s and a third dim\n",
    "# (hxd) with h=8.\n",
    "# This rearrangement results in each batch which is now (nxh), with the same sequence length, and the number of channels d\n",
    "# has been reduced by a factor of 8.\n",
    "t2 = rearrange(t, 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42e15663-22ef-429a-855e-e3268777a612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can reverse the operations just as easily\n",
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)\n",
    "t2.shape, t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff5c5968-b225-4a52-94d3-b19664618719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming that rearrangements return the same results.\n",
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34797c48-99d6-4929-a76e-7a41f3d1abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads): # Adding an additional parameter nheads\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale  = math.sqrt(ni / nheads)\n",
    "        self.norm   = nn.BatchNorm2d(ni)\n",
    "        self.qkv    = nn.Linear(ni, ni*3)\n",
    "        self.proj   = nn.Linear(ni, ni)\n",
    "\n",
    "    def forward(self, inp): \n",
    "        n, c, h, w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        # Take the number of heads (for demo purposes, 32 channels split over 4 heads i.e. 8 per head.)\n",
    "        # Each batch becomes 4 times larger due to the additional heads.\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads) \n",
    "        q, k, v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1, 2)) / self.scale\n",
    "        x = s.softmax(dim=-1) @ v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x).transpose(1, 2).reshape(n, c, h, w)\n",
    "        return x + inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fbb9b16-6124-4f3c-b62a-260b849d3d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4) # MH attention with 32 channels and 4 heads\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffcd951e-cf7d-4ecc-95ac-7319a85e5ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0146, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0098, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.mean(), sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3c9bb8-a3aa-41b2-a832-e7cd5e9272f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch already has this all built in with nn.MultiheadAttention.\n",
    "# Using batch_first=True, ensures that the first dimension passed is the batch so that everything is \n",
    "# aligned closely with Diffusers.\n",
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx, nmw = nm(t, t, t) # Q, K, V - Passing other projections enables cross attention\n",
    "nmx = nmx + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27d4cc37-fa5c-421c-92b9-6a4c35d1db7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0007, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0011, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmx.mean(), nmx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382012e-c76e-4b14-afa7-f8fd48db68ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
