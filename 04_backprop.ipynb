{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8578e7c9-e365-41ae-84d5-9309d45e603f",
   "metadata": {},
   "source": [
    "## **The Forward and Backward Passes of a Simple MLP / Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb44585-199f-4443-a686-f9f8a3ffda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, math, os, time, shutil\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch import tensor\n",
    "from fastcore.test import  test_close\n",
    "from pathlib import Path\n",
    "\n",
    "# Configs\n",
    "torch.manual_seed(42)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "# Path setup\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "# Loading MNIST data as tensors\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048eeac-ec99-4e49-a81e-dbd7dea1f229",
   "metadata": {},
   "source": [
    "## Building the Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64614a9e-b6fe-4282-a614-fc9e342f7705",
   "metadata": {},
   "source": [
    "### Basic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9729c1f-fa12-4727-b34d-7c2e0e8e0074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here n is the number of training examples and m is the number of pixels\n",
    "n, m = x_train.shape\n",
    "# C is the number of possible values of our outputs / digits\n",
    "c = y_train.max() + 1\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018961b-f0c4-4c26-8588-1a293db2e9c5",
   "metadata": {},
   "source": [
    "We will decide ahead of time what the number of our hidden **activations** or **nodes** for a single layer will be. Lets pick an arbitrary number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda0f8f4-f9e6-42cb-9d50-ce24ae00c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden activations\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e302f-1441-4009-8831-2f3e30ab856e",
   "metadata": {},
   "source": [
    "As we have already established, we will be utilizing what we've learnt about matrix multiplications to get our output probabilities for each input row. We already have training data in a **`50,000x784`** matrix. For the linear function to work, we will need weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bb6a51-0f57-43bc-b966-3b1866324209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight matrix will contain 50000x50 random values\n",
    "w1 = torch.randn(m, nh)\n",
    "# Adding the biases for summation operations to create the linear function.\n",
    "# Creating a matrix of 0s, one for each hidden activation\n",
    "b1 = torch.zeros(nh)\n",
    "# For layer two, we will start with nh inputs.\n",
    "# But, we will stick with 1 output column to simplify the loss calcs. Which will be MSE instead of cross entropy.\n",
    "w2 = torch.randn(nh, 1)\n",
    "#  The sample applies for the bias i.e. sticking with a single output for simplicity.\n",
    "b2 = torch.zeros(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f59504e-6f58-42e3-aecc-df4dcecd35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple linear function for putting X through a single layer\n",
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d057b0-8716-484f-b310-08b24ff83565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the shape of our validation matrix\n",
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5d5371-63e8-4d2a-a450-e07ecc8ef608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing the validation data through the linear function will result in a 10000x50 matrix\n",
    "t = lin(x_valid, w1, b1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e20bff-35f3-4e29-8809-39382041cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the results through a ReLU\n",
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7979256d-7b3b-4a7c-862a-b7c628248286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n",
       "        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n",
       "        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n",
       "        ...,\n",
       "        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n",
       "        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n",
       "        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45780640-b7ba-43b4-a050-9e9d651060d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic MLP which takes a mini-batch of data\n",
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc69a2fe-5fc9-425a-a013-162d441ed178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model will now output a single column output\n",
    "# Again, this is only for simplicity since we will be testing the model using MSE.\n",
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea27bb9d-6c92-49de-883d-7cc56f3a703a",
   "metadata": {},
   "source": [
    "### Simple Loss Function: Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6184157-4991-4dde-8caa-c117f481c105",
   "metadata": {},
   "source": [
    "MSE is not a suitable loss function for multi-class classification. **We are over simplifying things for demonstration purposes only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb59cb20-4a23-4bcf-98d6-97cb9a083696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17822e7-5929-4eea-8cbf-5cdf77d77f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res - y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d483a-5cfd-4a9a-be32-db5b7d7af478",
   "metadata": {},
   "source": [
    "The calculation above is not correct. At the moment, based on broadcasting rules, each of our 10000 row items will be subtracted by the 10000 items in the array. PyTorch will automatically insert a unit x-axis in `y_valid`. This is not the shape of outputs that we desire!\n",
    "\n",
    "For correct element-wise operations, we will get rid of the trailing unit axis in `res`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7600f12c-a85e-4e36-9aec-cfbb55f3da4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One option is to use indexing to remove the trailing dimension\n",
    "res[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee04a63f-e572-4809-90d1-0cf676d7566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can use squeeze() to drop all trailing unit dimensions\n",
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc6e54f8-3a05-4d06-ac7b-448d48a287b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10000, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo: Adding arbitrary unit dimensions\n",
    "test = res[None, :, None, None]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210b8465-4f80-411b-9a62-6f640471d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using squeeze()\n",
    "test.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae533178-805f-4e9a-816d-c34f721a19de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our results will now have the correct shape\n",
    "(res.squeeze() - y_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "731488af-cec0-4674-98f8-33f9c6322077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets calculate our predictions for the training set\n",
    "# Converting to float for MSE\n",
    "y_train, y_valid = y_train.float(), y_valid.float()\n",
    "\n",
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430497b2-f3bd-4dbb-a0d9-06e26c35ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an MSE function\n",
    "# Alternatively, we can also use output[:, 0] instead of squeeze as we have already established\n",
    "def mse(output, target): return (output.squeeze() - target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7780ae8d-bd7c-4194-9b5e-d9f9b0d6be2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4308.76)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9414ed-0194-4cae-954a-766e397b74f2",
   "metadata": {},
   "source": [
    "### Calculating Our Gradients and Setting Up the Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0928f-106d-4cfe-a9e7-ee2b9ed20072",
   "metadata": {},
   "source": [
    "We will need our model to learn over a certain number of steps / epochs and this isn't going to be a one-off prediction. The neural network will need to calculate the gradient of the loss with regard to the weights of our model, in an iterative manner, to update the weights and biases of our model based on the direction of the gradient.\n",
    "\n",
    "This is effectively an application of the chain rule. And when these derivatives are carried out in a matrix then that matrix is also called a Jacobian.\n",
    "\n",
    "We can calculate derivatives symbolically using [SimPy](https://simpy.readthedocs.io/en/latest/topical_guides/simpy_basics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3cb5d62-064f-41a2-84b2-80ea72d68f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "x, y = symbols('x y')\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38094162-7273-4e1c-b960-1fa87fa9c700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x + 4$"
      ],
      "text/plain": [
       "6*x + 4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(3*x**2 + 4*x + 9, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f5a689a-c8ee-40a4-99ba-aff68f133889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the python debugger to explore the outputs of the functions\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a861b6-cddf-41ae-8fe5-3c45d977145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the Linear Model function\n",
    "def lin_grad(inp, out, w, b):\n",
    "    # Grad of matmul with respect to input\n",
    "    # Storing the gradients of our inputs, gradients of outputs wrt inputs\n",
    "    inp.g = out.g @ w.t()\n",
    "    #pdb.set_trace()\n",
    "    # Gradient of the outputs wrt the weights will be sum of inputs*outputs\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    # Gradient of the bias\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f93e2739-a044-4c52-8ae4-39969b9c5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the func for our forward and backward pass\n",
    "def forward_and_backward(inp, targ):\n",
    "    # Forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:, 0] - targ # or unsqueeze()\n",
    "    loss = diff.pow(2).mean()\n",
    "\n",
    "    # Backward pass:\n",
    "    # The attribute out.g will allow us to store the gradients in the layer itself\n",
    "    # Also, due to the chain rule, we will have a number of intermediate calculations in the\n",
    "    # backward pass.\n",
    "    out.g = 2.*diff[:, None] / inp.shape[0] # Derivative of the loss wrt to the NN.\n",
    "    lin_grad(l2, out, w2, b2) # Calling intermediate calc from the forward pass.\n",
    "    l1.g = (l1 > 0).float() * l2.g # Another intermediate call\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbed60-f12d-4671-bc50-96269c36b05a",
   "metadata": {},
   "source": [
    "In the code block above, the **Backward Pass** components are :\n",
    "- `out.g = 2.*diff[:, None] / inp.shape[0]` calculates the gradient of the loss wrt `out` and comes from the formula for derivative of the Mean Squared Error (MSE) function.\n",
    "- `lin_grad(l2, out, w2, b2)` compute the gradients for the second layer.\n",
    "- `l1.g = (l1 > 0).float() * l2.g` computes the gradient of ReLU activation which is 1 for positive inputs and 0 for negative inputs. `l1 > 0` creates a mask of 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66ea9e68-8922-4330-8ef5-1529b6a0c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling forward and backward\n",
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57a1e5-d0dd-43fc-a738-606140863a28",
   "metadata": {},
   "source": [
    "This section contains some really \"clunky\" code and could do with some optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67abc224-6a90-47f7-b3c6-c667ec42732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing\n",
    "def get_grad(x): return x.g.clone()\n",
    "\n",
    "chks = w1, w2, b1, b2, x_train\n",
    "grads = w1g, w2g, b1g, b2g, ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2321d54c-4f7b-452c-b9c6-dde1ed7f121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use PyTorch autograd to check our results\n",
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "\n",
    "ptgrads = w12, w22, b12, b22, xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bd7f2ae-60e8-4f17-ae79-0cd6f4534a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15f412fa-0992-4eb3-b30f-f55c21124276",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49679228-8252-4d32-8340-5be79d026110",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f0f8a-15fd-4c4d-b143-b1d3e424b694",
   "metadata": {},
   "source": [
    "## Refactor the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616840ff-e06b-44df-99c9-b995b2321279",
   "metadata": {},
   "source": [
    "### Layers as Classes\n",
    "\n",
    "We can refactor our code to include classes with callable objects to minimize manual tinkering with the various components of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6418f86-a0ad-464c-b98c-a8004aa40baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    # This class will calculate ReLU for each forward and backward pass on top of storing the outputs\n",
    "    def __call__(self, inp): # Forward pass\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # Calling the stored input gradients\n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "705eab97-c603-423b-a235-adfe7d456f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    # Storing the weights and biases for Linear calcs\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        # Calling the linear function from before. This approach allows us to stack funcs within calls.\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t() # .T can be used interchangeably\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4fd629d-5009-494d-8d61-df9ab88580a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        # Storing the input and the target\n",
    "        self.inp, self.targ = inp, targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9779f779-ebf1-4cd1-98f7-1b7fa6c47329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        # Storing the model's layers i.e. instances of classes\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = Mse()\n",
    "\n",
    "    def __call__(self, x, targ):\n",
    "        # Calling each layer\n",
    "        for l in self.layers: x = l(x)\n",
    "        # HuggingFace puts their loss inside the forward pass\n",
    "        # FastAI, and others, use separate functions for loss calculations\n",
    "        return self.loss(x, targ)\n",
    "\n",
    "    def backward(self):\n",
    "        # loss is MSE() which already has inputs and targets stored.\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4663c108-dcbe-4f57-9662-974274c49fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3c0c1ae-b9d0-4db2-af22-bbaeff6dd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c551fe7-950c-4b0a-bec4-446c62a51f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ea47620-8cb9-4c92-b381-317eddd7c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the gradients were stored earlier\n",
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05779cb2-0483-4f21-b51b-f635aa55c7bd",
   "metadata": {},
   "source": [
    "## Module.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e233b5b-0d84-4038-8596-f24fd8d36b10",
   "metadata": {},
   "source": [
    "In the previous section, our code has numerous repititions for e.g. `self.inp` and `self.out` are being called a bunch of times.\n",
    "\n",
    "The Module class will allow us to simplify the code even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14de1baf-86cb-4c55-97ed-de963ccfd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(): # Module will be an inherited class\n",
    "    def __call__(self, *args): # *args stores arguments in a list\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "\n",
    "    def backward(self): self.bwd(self.out, *self.args) \n",
    "\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d01bdb-7dd2-4e51-b011-931874316342",
   "metadata": {},
   "source": [
    "In the block above `*self.args` unpacks elements of `self.args` (which is expected to be an iterable like a list or tuple) as arguments. \n",
    "\n",
    "With the object inheritance, we are able to clean up the Relu() module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90ec00f7-5466-4052-8788-28d22fb54ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "\n",
    "    def bwd(self, out, inp): inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d5c80-edbf-4be2-80c8-03b6682d1eb5",
   "metadata": {},
   "source": [
    "Same for Lin() and MSE()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c6b7a44-fb83-4055-9a15-71da4f5eafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w, self.b = w, b\n",
    "\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051b5a6-d3fd-4927-81a8-f1cae3ddf355",
   "metadata": {},
   "source": [
    "Sometimes, at the cost of some memory, we can further optimize calculations by storing partial results.\n",
    "\n",
    "Notice that `(inp.squeeze() - targ)` is being called twice and can be stored in an object and called directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8d3ba10-6b28-430b-b236-d6ad9ead3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward(self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "\n",
    "    def bwd(self, out, inp, targ): inp.g = 2.*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d777237-7ba2-4364-ae8f-e7a9f1acdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e97724c-2dd5-4543-99ed-cd21fa117010",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc587a21-2b1e-4ce9-9a35-d3ff7f79afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64ae2bc5-e96c-4cb6-834c-a5a9b353acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f71b73-46d1-4150-998a-e4c1662fefca",
   "metadata": {},
   "source": [
    "## Autograd Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a5293-cc7a-4183-91a9-9dcccc9d1377",
   "metadata": {},
   "source": [
    "Having built out the classes in previous sections, we've satisfied the course's conditions regarding abstractions.\n",
    "\n",
    "PyTorch already has things built for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebd60de7-bc24-437c-9805-dee905977a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f814ade-0862-4fbb-9b90-e0d1c77d48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's Module class.\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        # As before, lets generate our random weights\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb717-46db-4b88-a343-608a6c7b1144",
   "metadata": {},
   "source": [
    "We won't need to create `backwards()` since PyTorch will handle the storage of derivatives for us using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ea68955-233c-4c54-896f-d0bbcbf3cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in, nh), nn.ReLU(), Linear(nh, n_out)]\n",
    "\n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        # Using PyTorch's mse_loss()\n",
    "        return F.mse_loss(x, targ[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8802566-1874-4d5a-ab42-f33f1659c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14b9010c-b12e-43f3-b295-a5e256ec6f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n",
       "          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n",
       "        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n",
       "        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99fcd1-1d8e-4566-a2ba-fe4cc73d1ea1",
   "metadata": {},
   "source": [
    "Et Voila!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
