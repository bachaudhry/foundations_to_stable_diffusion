{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187828e5-96fe-401b-9fe3-dc73b878ae0e",
   "metadata": {},
   "source": [
    "# **Training on LSUN (Large Scale Understanding) Bedrooms Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96f943-6f37-419e-950b-6787bed18f78",
   "metadata": {},
   "source": [
    "In this NB, we will take on the more challenging task of generating larger images. The dataset being used here is the [LSUN Bedrooms Dataset](https://paperswithcode.com/sota/image-generation-on-lsun-bedroom-256-x-256), which is a subset of the original comprising 10 classes of objects / scenes. The description for the original data is as follows:\n",
    "\n",
    "> The Large-scale Scene Understanding (LSUN) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c15f6-7d46-4d80-96c1-60e56e9f43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from miniai.imports import *\n",
    "from miniai.diffusion import *\n",
    "from diffusers import UNet2DModel, UNet2DConditionModel, AutoencoderKL\n",
    "from fastprogress import progress_bar\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "import timm\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea903cc7-b368-45da-a744-ef2519437589",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577d620-1eed-4cbf-ae99-ff0729e33399",
   "metadata": {},
   "source": [
    "## **Download and Process Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c60734-a6b9-4ec0-a96b-822ce347b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path = path_data/'bedroom'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471a56-d1f6-47db-9279-80e177c6178b",
   "metadata": {},
   "source": [
    "Given persistent issues with downloading the data, Jeremy placed a subset of 20% of the data on AWS. Also, the original data is stored in a LMDB format which Jeremy converted for us as well.\n",
    "\n",
    "`NOTE` - _If the download takes a long time in Python, then revert to using a shell._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9c5fb-842b-4095-98f3-7eed9c2f804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3.amazonaws.com/fast-ai-imageclas/bedroom.tgz' # Download tarball\n",
    "if not path.exists():\n",
    "    path_zip = fc.urlsave(url, path_data)\n",
    "    shutil.unpack_archive('data/bedroom.tgz', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1302f9b-c8d9-4187-857f-bafa397fd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e442329-0c2a-4cb7-9f18-7af298e6d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_image is highly optimized for this op.\n",
    "# .RGB ensures conversion of image types to required outputs.\n",
    "def to_img(f): return read_image(f, mode=ImageReadMode.RGB)/255 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1f41b7-9645-4aa2-8664-b2b52139c2e6",
   "metadata": {},
   "source": [
    "Lets work on converting the images to latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cef2b4-df90-4f55-901d-8da280f3ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDS:\n",
    "    def __init__(self, spec):\n",
    "        self.path = Path(path)\n",
    "        self.files = glob(str(spec), recursive=True) #search for filetypes\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i): return to_img(self.files[i])[:, :256, :256] # crop last dims of images to reduce some compute and align sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c39dde-f9e4-4fad-a807-57ec9431ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImagesDS(path/f'**/*.jpg') # search recursively for all jpeg files in the bedroomm folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f7894-9dfb-40c3-a6ad-cd457d83976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=bs, num_workers=fc.defaults.cpus) # Load batches with max cpus in parallel.\n",
    "xb = next(iter(dl))\n",
    "show_images(xb[:16], imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677a1cf-04a1-4ae4-bb3a-706bb8798249",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb[:16].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ae9f6-fbc4-417b-a859-4dd6b1ecb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of floats.\n",
    "16*3*256*256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04155b0c-bcf7-4d36-aaf0-1d70306298e5",
   "metadata": {},
   "source": [
    "## **Using a Pre-Trained VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596eda2-00c4-40ed-945a-664336cddc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing pretrained encoder.\n",
    "# Turn of gradient computations inplace\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b210ad2-e382-4488-a30a-43fdfd2dd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = vae.encode(xb.cuda()) # encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a416d9-90f1-430b-a7d1-29a0c6345bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = xe.latent_dist.mean[:16]\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb3ed6-fb40-4402-a57e-ec17a74f9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of original vs compressed / encoded image sizes.\n",
    "(16*3*256*256) / (16*4*32*32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e231ae-f7ef-439b-8f17-7ccff43ba860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab images and first three channels\n",
    "# Sigmoid ensures images fall between 0 and 1\n",
    "show_images(((xs[:16, :3]) / 4).sigmoid(), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407f242-2be4-4fbd-8edb-8a3277a1525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = to_cpu(vae.decode(xs)) # Decode tensor\n",
    "show_images(xd['sample'].clamp(0, 1), imsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc597b-838b-4889-a9bc-f24d47b740b0",
   "metadata": {},
   "source": [
    "Reconstructed images for quality checks before passing them to the diffusion model. The [`sd-vae-ft-ema`](https://huggingface.co/stabilityai/sd-vae-ft-ema) VAE has a known limitation in regenerating writing / text in images. The creators mention:\n",
    "\n",
    "> We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces. The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights.\n",
    ">\n",
    "> It uses the same loss configuration as the original checkpoint (L1 + LPIPS). The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU). To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.\n",
    "\n",
    "Based on Perplexity, these are some additional details:\n",
    "\n",
    ">   **Training Data**: It was fine-tuned on a combination of the LAION-Aesthetics and LAION-Humans datasets to enhance the reconstruction of faces and human subjects.\n",
    ">\n",
    ">   **Loss Configuration**: The model uses the same loss configuration as the original kl-f8 autoencoder, which includes L1 loss and LPIPS (Learned Perceptual Image Patch Similarity).\n",
    ">\n",
    ">   **Exponential Moving Average (EMA) Weights**: The ft-EMA version utilizes EMA weights, which help stabilize the training process and improve model performance.\n",
    ">\n",
    ">   **Training Steps**: The model was trained for 313,198 steps.\n",
    ">\n",
    ">   **Performance**: Compared to the original kl-f8 VAE, the ft-EMA model shows slightly improved performance, with a lower rFID score of 4.42 versus 4.99 for the original.\n",
    ">\n",
    ">   **Applications**: It can be used as a drop-in replacement for the original autoencoder in the Stable Diffusion pipeline, potentially leading to improved downstream generation results. Additionally, it is suitable for tasks like image compression and editing.\n",
    ">\n",
    ">   **Variants**: There is another variant, sd-vae-ft-mse, which emphasizes MSE reconstruction and produces smoother outputs.\n",
    "\n",
    "\n",
    "To read more about Learned Perceptual Image Patch Similarity (LPIPS) read the [paper and the visit the associated Github page](https://richzhang.github.io/PerceptualSimilarity/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f1a3a-eb51-465a-8ad7-939c11f2947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Memory Mapped Numpy File (NPMM) format to handle in memory tasks more efficiently.\n",
    "mmpath = Path('data/bedroom/data.npmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70f88f-4766-49bb-b2fd-87d01c7f83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae6ff3-7df2-486b-a5cf-c81c2ff4dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmshape = (303125, 4, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ff0a1-8701-4b8a-9444-7138fe5f9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mmpath.exists(): # Create and store npmm file on disk. Shapes are the same as our images.\n",
    "    a = np.memmap(mmpath, np.float32, mode='w+', shape=mmshape)\n",
    "    i = 0\n",
    "    for b in progress_bar(dl): # Grab a mini batch\n",
    "        n = len(b)\n",
    "        # Encode and get the means of the latents and convert to numpy since pytorch doesn't have a memory mapping tool (as of 2023)\n",
    "        a[i : i+n] = to_cpu(vae.encode(b.cuda()).latent_dist.mean).numpy()\n",
    "        i += n\n",
    "    a.flush() # Ensure that the contents of the cache are written to disk.\n",
    "    del(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d5e78-e25e-4041-b963-c35810f27ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.memmap(mmpath, dtype=np.float32, mode='r', shape=mmshape) # apply memory mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7fe55-6e2f-44bc-b4a8-1623b6d604b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor(lats[:16]) # Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6ba1d-caca-4791-a35d-e3b8e30f95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = to_cpu(vae.decode(b.cuda()))\n",
    "show_images(xd['sample'].clamp(0,1), imsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae1540-0869-42f2-9467-9f08479971bc",
   "metadata": {},
   "source": [
    "## **Noisify**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafea569-b712-4b4f-abf6-eb009f569b93",
   "metadata": {},
   "source": [
    "We are now able to apply pipeline operations such as Noisify using numpy, which is pretty cool!! These foundational concepts should be applied more regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190474dc-b563-4eef-a896-82a52b98c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ddpm(b): return noisify(default_collate(b)*0.2) # Ensure unit standard dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d96fe-cb2b-44f9-80a9-dcb246c42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416decfa-32bb-4e2c-a951-6b3a9f459d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets.\n",
    "tds = lats[:n // 10*9] # First 90%\n",
    "vds = lats[ n // 10*9:]# Last 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48b4a8-47c1-471c-b62f-d43f60283085",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77fba5-7c7c-4184-9cea-c7b835edce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(*get_dls(tds, vds, bs=bs, num_workers=fc.defaults.cpus, collate_fn=collate_ddpm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4ff75-a9bf-4b4c-ac19-bc8077c5db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xt, t), eps = b = next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2838a-0000-49b7-aa5b-67f38981c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xt[:9, 0], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d82665-77af-4dfc-8a73-af54013e6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "xte = vae.decode(xt[:9].cuda()*5)['sample']\n",
    "show_images(xte.clamp(0,1), imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4a42a-b903-42c8-8705-1fa4d1e4659d",
   "metadata": {},
   "source": [
    "## **Train Latent Diffusion Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f980073-2511-4f2d-b984-97304af3e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ddpm(model):\n",
    "    for o in model.downs:\n",
    "        for p in o.resnets: p.conv2[-1].weight.data.zero_()\n",
    "\n",
    "    for o in model.ups:\n",
    "        for p in o.resnets: p.conv2[-1].weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbefd7-ab9b-41f8-b9ea-61054f64894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "epochs = 25\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = EmbUNetModel(in_channels=4, out_channels=4, nfs=(128, 256, 512, 768), num_layers=2,\n",
    "                     attn_start=1, attn_chans=16)\n",
    "init_ddpm(model)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07773707-7e4d-4270-b7e2-48a2f4a6977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs) # Loss will be higher since the model tries to generate latent pixels which is a much more difficult task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b11526-5fbb-43a1-9945-8afc47ce7e01",
   "metadata": {},
   "source": [
    "## **Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fac6d-f35e-4e81-87b6-ba961bdb1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (16,4,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a1bd8-912d-4723-9b9e-3f8ff33e2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=100, eta=1., clamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19715180-ab67-4a1b-ae44-056c05e31cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = preds[-1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67811fa7-6c51-47e5-9633-1e1ae69349a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode since what we're predicting is latents.\n",
    "with torch.no_grad(): pd = to_cpu(vae.decode(s.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a1da7-f9ae-4702-8b89-683b403c43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(pd['sample'][:9].clamp(0,1), imsize=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
