{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5ecc7f-5858-42c8-b51f-da090949b406",
   "metadata": {},
   "source": [
    "# **Conditional Diffusion With Multi-Headed Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cd99a-39d8-4966-b680-8ace43af7317",
   "metadata": {},
   "source": [
    "Most of this NB is a copy of `24_diffusion_unet`.\n",
    "\n",
    "The key differences come from the introduction of **Multi-Headed Attention** and components related to **Conditional Diffusion**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e7b51-2277-418c-8490-5f56ef9c0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5288931-c6e0-4859-856f-c60dc60f6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miniai.imports import *\n",
    "\n",
    "from einops import rearrange\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f623bef-1eac-435b-8e14-df4f82430276",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10b521-9f0e-47fd-b172-4d0ab222d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 512\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fdb4e-480b-4261-b8a7-7d8ea7dbf022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key difference with the original NB i.e. number 24 is that we are reverting to cosine scheduling due to its\n",
    "# superior results over Karras et al.\n",
    "def abar(t): return (t*math.pi/2).cos()**2\n",
    "\n",
    "def inv_bar(x): return x.sqrt().acos()*2 / math.pi\n",
    "\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.rand(n,).to(x0).clamp(0, 0.999)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt() * ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl])\n",
    "\n",
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6e297-86f1-4eaa-8a79-326ab59ac8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "dl = dls.train\n",
    "(xt,t), eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f875b2-96e9-466f-8004-5aef956da114",
   "metadata": {},
   "source": [
    "## **Train Model (with updated code blocks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5b607-5381-4a8b-944e-91051af63fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(tsteps, emb_dim, max_period=10000):\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:, None].float() * exponent.exp()[None, :]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0, 1, 0, 0)) if emb_dim%2==1 else emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018104a-d229-4661-ba43-6eda5b8abe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming unet_conv to pre_conv\n",
    "def pre_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b3efd-f40e-4804-9111-8942902b183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da818d-f255-4106-b2f0-2811aa5707cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-activation linear version\n",
    "def lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4201d-4e9e-464e-b11a-d266b3a1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problematic version, skip in favour of the next cell block\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(ni, ni//attn_chans, batch_first=True)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.attn(x, x, x, need_weights=False)[0]\n",
    "        return x.transpose(1,2).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5051c55-086b-4a60-a1dc-25b1461b57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 1-D self attention since we're not using .view() in forward()\n",
    "# This attention is not ResNet specific \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans # number of attention channels\n",
    "        self.scale = math.sqrt(ni/self.nheads) # for scale, divide by the effective number of heads\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68228c22-c1b5-4bc1-841e-06b5d429cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit converts 1-D attention to 2-D and back.\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9feb5-6971-40a7-a6a3-ae0d39c368b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = pre_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = pre_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "        self.attn = False\n",
    "        # If attention is called then we can create a SelfAttention2D layer.\n",
    "        if attn_chans: self.attn = SelfAttention2D(nf, attn_chans)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        scale,shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x) # Add attention\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e8b37-59fd-4698-870a-d30a598f46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saved(m, blk):\n",
    "    m_ = m.forward\n",
    "\n",
    "    @wraps(m.forward)\n",
    "    def _f(*args, **kwargs):\n",
    "        res = m_(*args, **kwargs)\n",
    "        blk.saved.append(res)\n",
    "        return res\n",
    "\n",
    "    m.forward = _f\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16312743-9ec1-496c-8c21-ef243252c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module): # Adding attn_chans\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans), self)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: x = resnet(x, t)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f83ff-c1aa-46c1-8d8f-d3275c19f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module): # Same case with upblock\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)\n",
    "            for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79134438-192a-4734-9a4d-9ac2c4c7c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbUNetModel(nn.Module): \n",
    "    # In addition to attn_chans, we need to specify the index block where attn needs to be applied first. So we add attn_start.\n",
    "    # Attention is never applied to the initial (0) layer as it can explode memory usage. Another approach is to specify the grid\n",
    "    # size where attention needs to begin. \n",
    "    def __init__(self, in_channels=3, out_channels=3, nfs=(224, 448, 672, 896), num_layers=1, attn_chans=8, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs=[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        n = len(nfs)\n",
    "        for i in range(n):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            # No attention if start cond. isn't met.\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=n-1, num_layers=num_layers,\n",
    "                                        attn_chans=0 if i<attn_start else attn_chans))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1]) # Missing attention!\n",
    "        \n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(n):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=n-1, num_layers=num_layers+1,\n",
    "                                    attn_chans=0 if i>=n-attn_start else attn_chans))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp): # Forward remains the same.\n",
    "        x, t = inp\n",
    "        temb = timestep_embeddings(t, self.n_temb)\n",
    "        emb = self.emb_mlp(temb)\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf4c39-23f9-47e9-8d99-3a21ac8eb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b71da-9fa9-43a2-a73b-0e74d057a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7f17f-b470-487b-99fd-1e27d7bb9e0b",
   "metadata": {},
   "source": [
    "TODO: A note on model performance \n",
    "\n",
    "Tanishq had an interesting observation regarding the spatial mixing aspects of multi-headed attention i.e. using attention very early on explodes memory usage due to the model's emphasis on individual pixels. However, we do need attention to be applied _early enough_ to do the required mixing of spatial information to improve performance. The art is in finding the optimal balance, as with all things, between targetted early mixing and memory conservation.\n",
    "\n",
    "Johno followed up with a great point (there are so many of these going around throughout the course) on a trick used in vision transformers / diffusion transformers, which requires the flattening of an `8x8` patch of an image using a convolutional block to a `1x1xNC` (NC being a large number of channels). This reduces the spatial information by increasig the number of channels, and gets the image down to a manageable size where the application of attention doesn't become as memory intensive.\n",
    "\n",
    "Another trick is **Patching**, which Perplexity defines as:\n",
    "\n",
    "> Patching is a crucial technique in the processing of images for neural networks, particularly in architectures like Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs). This method involves segmenting an image into smaller, manageable pieces or \"patches,\" which are then transformed into embeddings suitable for further processing by the network. \n",
    "\n",
    "> ### 1. Purpose of Patching\n",
    "> \n",
    "> The primary purposes of patching in neural networks include:\n",
    ">\n",
    ">    **Image Segmentation**: Patching divides an image into smaller sections, allowing the model to focus on localized features rather than processing the entire image at once. This segmentation is vital for capturing detailed information that might be lost in larger contexts.\n",
    "> \n",
    ">    **Dimensionality Transformation**: Each patch is transformed into a lower-dimensional representation (embedding). This transformation helps in reducing computational complexity and allows for more efficient processing by the neural network.\n",
    "> \n",
    ">    **Normalization**: After embedding, layer normalization can be applied to stabilize training and improve convergence rates, ensuring that the embeddings maintain consistent statistical properties throughout training.\n",
    "> \n",
    "> ### 2. Patch Embedding Process\n",
    "> \n",
    ">The process of creating patch embeddings typically involves several steps:\n",
    ">\n",
    ">    **Patch Extraction**: The input image is divided into non-overlapping patches. For example, an image might be split into `16×16` pixel patches. If the original image size is `224×224`, this results in 196 patches.\n",
    ">\n",
    ">    **Flattening and Projection**: Each patch is flattened into a one-dimensional vector and then projected into a higher-dimensional space using a linear transformation (often implemented as a convolution operation). This projection creates a fixed-size embedding for each patch.\n",
    "> \n",
    ">    **Adding Positional Information**: Since transformers do not inherently understand spatial relationships, positional embeddings are added to each patch embedding to retain information about the original spatial arrangement of patches within the image.\n",
    ">\n",
    "> ### Applications and Benefits\n",
    "> \n",
    ">    **Improved Feature Learning**: By focusing on smaller patches, models can learn more nuanced features that contribute to better performance in tasks such as image classification and segmentation2.\n",
    "> \n",
    ">    **Scalability**: Patching allows models to handle larger images without requiring disproportionate increases in computational resources. It enables efficient training on high-resolution images by breaking them down into smaller components.\n",
    "> \n",
    ">    **Adaptability to Different Resolutions**: Techniques like Multi-Scale Patch Embedding (MSPE) enhance model adaptability to varying input resolutions by using multiple variable-sized kernels, which can be particularly beneficial in real-world scenarios where image sizes are inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912384a-a187-4ae7-ba25-7f7a44e13c85",
   "metadata": {},
   "source": [
    "## **Sampling (same as before)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee6c1f-5f80-4640-a408-6623b9d66bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miniai.fid import ImageEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882066c0-1f74-4f8f-b918-c640b7f8b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel = torch.load('models/data_aug2.pkl')\n",
    "del(cmodel[8])\n",
    "del(cmodel[7])\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n",
    "\n",
    "bs = 2048\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n",
    "\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "\n",
    "ie = ImageEval(cmodel, dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112f833-6649-4643-bcf1-e30a85548199",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (2048,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98929ac2-788c-49a7-94c5-eb5e43e9a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig, clamp=True):\n",
    "    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n",
    "    x_0_hat = ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt())\n",
    "    if clamp: x_0_hat = x_0_hat.clamp(-1,1)\n",
    "    if bbar_t1 <= sig**2+0.01: sig=0.  # set to zero if very small or NaN\n",
    "    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n",
    "    x_t += sig * torch.randn(x_t.shape).to(x_t)\n",
    "    return x_0_hat, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe44f23-0ce3-4864-b58b-f625b02a8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(f, model, sz, steps, eta=1., clamp=True):\n",
    "    model.eval()\n",
    "    ts = torch.linspace(1 - 1/steps, 0, steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        noise = model((x_t, t))\n",
    "        abar_t1 = abar(t-1/steps) if t>=1 / steps else torch.tensor(1)\n",
    "        x_0_hat, x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1 - ((i+1)/100), clamp=clamp)\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb915f3-27c5-4db8-9cbf-98e28cf170f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=100, eta=1.)\n",
    "s = (preds[-1] * 2)\n",
    "s.min(), s.max(), s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2098b0-7b4d-4652-b2d5-06025960710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(s[:25].clamp(-1,1), imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711b867-2958-417c-acde-75e63b34262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3e3ba-6027-4047-9346-4bc8ccf3ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=100, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc2ced-2e1c-49c2-b6e2-5d6f50943e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=50, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf728e7-48e5-4325-ba0c-45df024e0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=50, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812adbe1-222a-4065-be30-90e98d6825e8",
   "metadata": {},
   "source": [
    "## **Moving To A Conditional Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f689d8-38d7-4e57-9f1f-96ee9a1130a8",
   "metadata": {},
   "source": [
    "We will now have the ability to specify which types / classes of outputs our model should generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24e821-fe23-4d92-9f47-12355224a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ddpm(b):\n",
    "    b = default_collate(b)\n",
    "    (xt, t), eps = noisify(b[xl])\n",
    "    # The y-label yl will allow us to set the category of outputs\n",
    "    return (xt, t, b[yl]), eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bedac-b353-4fff-b8a4-b99f852c0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "dl = dls.train\n",
    "(xt,t,c),eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a9858-52c7-4065-8e0c-e6ccc74034e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondUNetModel(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels=3, out_channels=3, nfs=(224, 448, 672, 896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf * 4\n",
    "        # Convert the labels into a embedding vector, n_emb will ensure it is te same size as the timestep embedding\n",
    "        self.cond_emb = nn.Embedding(n_class, n_emb)\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Inputs now contain the label (0-9 for fashion mnist), in addition to the activations and timesteps\n",
    "        x, t, c = inp\n",
    "        temb = timestep_embedding(t, self.n_temb) # timestep embedding as before\n",
    "        cemb = self.cond_emb(c) # labels passed to conditioned embedding\n",
    "        emb = self.emb_mlp(temb) + cemb # time embedding passed to the mlp, then added with cemb resulting in a joing embedding\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7ce40-86e0-4e8d-812a-7e1986f797d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = CondUNetModel(10, in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c73bff-bea7-4773-8f36-8d574e73f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b53b01-aad8-4568-9382-8944267b07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (256,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe079d-da4c-4f3a-b676-17881b14bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cond_sample(c, f, model, sz, steps, eta=1.):\n",
    "    ts = torch.linspace(1-1/steps,0,steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    # Create a vector containing the conditional target, repeated across the batch\n",
    "    c = x_t.new_full((sz[0],), c, dtype=torch.int32)\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        # The model learns to denoise by type c\n",
    "        noise = model((x_t, t, c)) # Pass the c vector to the model\n",
    "        abar_t1 = abar(t-1/steps) if t>=1/steps else torch.tensor(1)\n",
    "        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100))\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda62b51-e1f8-4cc3-bc4a-529940cd1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = dsd['train'].features[yl].names\n",
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d48bde-0057-4d08-82ff-fc4dc9cfd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cid = 0\n",
    "preds = sample(cid, ddim_step, model, sz, steps=100, eta=1.)\n",
    "s = (preds[-1]*2)\n",
    "show_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab05f91-d242-4af1-a9c3-fc6a6239bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cid = 0\n",
    "preds = sample(cid, ddim_step, model, sz, steps=100, eta=0.)\n",
    "s = (preds[-1]*2)\n",
    "show_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
