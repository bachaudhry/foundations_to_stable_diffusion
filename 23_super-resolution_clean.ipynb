{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7cdaaf-c461-4655-b898-f189a801f02b",
   "metadata": {},
   "source": [
    "# **Tiny Imagenet - Super Resolution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b87ed-7e98-4ac7-8ddd-7d1c56bd3a95",
   "metadata": {},
   "source": [
    "We will be working on super-resolution and not classification tasks in this NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d95cd-aaed-40b6-ac5b-0401ae11809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, timm, torch, random, datasets, math, fastcore.all as fc \n",
    "import numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from fastcore.foundation import L\n",
    "from torch import nn,tensor\n",
    "from datasets import load_dataset\n",
    "from operator import itemgetter\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from functools import partial\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *\n",
    "from miniai.accel import *\n",
    "from miniai.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8fac2-b5c6-4e88-a27d-245b7f0f863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar\n",
    "from glob import glob\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eabb4a-86ad-4a90-a2c2-7cad8ab87872",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebf485-c885-48b4-9395-1319d794912e",
   "metadata": {},
   "source": [
    "## **Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc2b8a-370b-43c6-b557-0560e7e41e83",
   "metadata": {},
   "source": [
    "Copying over sections from the previous NBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f0fd5-5b0f-4dd8-a349-ad375507311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path = path_data/'tiny-imagenet-200'\n",
    "bs = 512\n",
    "xmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d13df2-7e4b-461f-aba1-fbb65ad88ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815b93b-964f-4a78-999f-a57adc1a221f",
   "metadata": {},
   "source": [
    "**For tasks like super-resolution (and image reconstruction in general) we must ensure that both the dependant and independant data receive the same amount of data augmentation.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefd11d-60cb-4690-bcec-2179ab9fcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(T.Pad(8), T.RandomCrop(64), T.RandomHorizontalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322f528-e3cc-464b-9cc3-b94a99048766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDS:\n",
    "    def __init__(self, path):\n",
    "        self.path = Path(path)\n",
    "        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        img = read_image(self.files[i], mode=ImageReadMode.RGB)/255\n",
    "        return tfms((img-xmean[:,None,None])/xstd[:,None,None])\n",
    "\n",
    "class TfmDS:\n",
    "    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        item = self.ds[i]\n",
    "        return self.tfmx(item),self.tfmy(item)\n",
    "\n",
    "def denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clamp(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4427f30-b77f-4aa1-abbe-cf67cb8428b3",
   "metadata": {},
   "source": [
    "- We will add random erasing just for the training set. This makes the task of learning more difficult for the model and forces it to learn to replace missing pixels.\n",
    "- In `TfmDS` the argument `tfmx` applies transformations / augmentations to the independant variable. In turn `tfmx()` applies `TF.resize()` to the 32x32 image, making it 64x64 and interpolates. This effectively doubles up the pixel count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8701e-83f9-4bf8-93fd-8d4fb48d231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add random erasing just for the training set. This makes the task of learning more\n",
    "# difficult for the model and forces it to learn to replace missing pixels.\n",
    "def tfmx(x, erase=True):\n",
    "    x = TF.resize(x, (32,32))[None]\n",
    "    x = F.interpolate(x, scale_factor=2)\n",
    "    if erase: x = rand_erase(x)\n",
    "    return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b0bc5-2113-4e32-a6c2-d7052a9d3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = TinyDS(path/'train')\n",
    "vds = TinyDS(path/'val')\n",
    "\n",
    "tfm_tds = TfmDS(tds, tfmx)\n",
    "tfm_vds = TfmDS(vds, partial(tfmx, erase=False))\n",
    "\n",
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa33f0f-1b99-4ba9-9ccd-408da862e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd56c97-fbe9-46d9-a8ea-adf081c9e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(xb[:4]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7acafc-82d4-46a0-893f-3ef9953a0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(yb[:4]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f61257-931f-40ef-8c33-a29c9805359e",
   "metadata": {},
   "source": [
    "## **Implementing a Denoising AutoEncoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418757b-63d5-4470-b487-a4976e721751",
   "metadata": {},
   "source": [
    "The `upblock` basically doubles the pixels through sequential upsampling. This is followed by an extra Resblock at the end to return 3 channels.\n",
    "\n",
    "So this model basically takes images which are squeezed into smaller representations, and then these representations are brought back up to their super-resolution target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652926e2-e9db-43a9-be67-a3741ca4fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upblock(ni, nf, ks=3, act=act_gr, norm=None):\n",
    "    return nn.Sequential(nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                         ResBlock(ni, nf, ks=ks, act=act, norm=norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189d15b-9139-4363-b2f4-07a5b36bf763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=act_gr, nfs=(32, 64, 128, 256, 512, 1024), norm=nn.BatchNorm2d, drop=0.1):\n",
    "    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) \n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [upblock(nfs[i], nfs[i-1], act=act, norm=norm) \n",
    "               for i in range(len(nfs)-1, 0, -1)]\n",
    "    layers += [ResBlock(nfs[0], 3, act=nn.Identity, norm=norm)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d624ca-52d1-4f80-b678-a00394047357",
   "metadata": {},
   "outputs": [],
   "source": [
    "iw = partial(init_weights, leaky=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5066d-13d4-4b7a-86ca-81621bbc7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB()\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n",
    "lr_cbs = [DeviceCB(), ProgressCB(), MixedPrecision()]\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee069e-8ec6-455e-876e-6a73c65bdcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learner(get_model().apply(iw), dls, F.mse_loss, cbs=lr_cbs, \n",
    "        opt_func=opt_func).lr_find(start_lr=1e-4, gamma=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0e577-66de-43c0-bd14-d91f67667f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 1e-3\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(get_model().apply(iw), dls, F.mse_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14000c69-b448-4cff-b2d1-29d0af7db63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750371c-c85e-4017-a0b4-44af560ac7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, targets and inputs extracted from the model\n",
    "p, t, inp = learn.capture_preds(inps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53328b-492e-4731-9fbc-6a0b9226e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(inp[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849c1d7-c3b9-452e-8fa7-2be60b727769",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(p[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b87ce2-ab6c-49f4-b212-e0e95a5322db",
   "metadata": {},
   "source": [
    "These results are quite bad!!\n",
    "\n",
    "The fundamental problem lies with the AutoEncoder, especially smaller AutoEncoders to carry out the task of super-resolution.\n",
    "\n",
    "Instead of adding additional complexity, we can instead simplify the model using the UNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4ce62-f494-4a2f-b4d2-0044a5fea28d",
   "metadata": {},
   "source": [
    "## **Switching Over To A UNET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f84db-624b-4be8-b64e-8e63225fcb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(learn)\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f984c5-e3f8-4b12-a3c6-d5b647f1c8d3",
   "metadata": {},
   "source": [
    "The [original paper](https://arxiv.org/abs/1505.04597) which introduced UNets in 2015, for use in medical imaging describes the architecture consisting... \n",
    "\n",
    ">.. of a contracting path to capture context and a symmetric expanding path that enables precise localization.\n",
    "\n",
    "We have referred to the \"contracting path\" as the **downsampling path** in previous NBs. Conversely, the \"symmetric expanding path\" is referred to as the **upsampling path**. The original paper used Convolution blocks in both upsampling and downsampling paths, whereas we have used ResNet blocks.\n",
    "\n",
    "During each stage of the upsampling path, the architecture copies over the activations from the opposite downsampling path. This ensures that the structure of the original image is retained for our super-resolution exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d3713-5ab9-45a6-a47e-0bef02f7855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyUnet(nn.Module):\n",
    "    def __init__(self, act=act_gr, nfs=(32, 64, 128, 256, 512, 1024), norm=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.start = ResBlock(3, nfs[0], stride=1, act=act, norm=norm)\n",
    "        # Downsampling path\n",
    "        self.dn = nn.ModuleList([ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "                                 for i in range(len(nfs)-1)])\n",
    "        # Upsampling path\n",
    "        self.up = nn.ModuleList([upblock(nfs[i], nfs[i-1], act=act, norm=norm)\n",
    "                                 for i in range(len(nfs)-1, 0, -1)])\n",
    "        # Final 3 channel block\n",
    "        self.up += [ResBlock(nfs[0], 3, act=act, norm=norm)]\n",
    "        self.end = ResBlock(3, 3, act=nn.Identity, norm=norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save first input before starting downsampling. \n",
    "        layers = []\n",
    "        layers.append(x)\n",
    "        x = self.start(x)\n",
    "        # For each layer in the downsampling path, save activations.\n",
    "        for l in self.dn:\n",
    "            layers.append(x)\n",
    "            x = l(x)\n",
    "        n = len(layers)\n",
    "        # During upsampling, instead of concatenating simply add the saved activations from \n",
    "        # the earlier steps\n",
    "        for i, l in enumerate(self.up):\n",
    "            if i!=0: x += layers[n-i] # Skip first layer.\n",
    "            x = l(x)\n",
    "        # At the end, at add back the first layer and pass through the ResBlock.\n",
    "        return self.end(x + layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1762d-907d-467f-b15c-c8011ddc21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero out the weights and biases of the model.\n",
    "def zero_wgts(l):\n",
    "    with torch.no_grad():\n",
    "        l.weight.zero_()\n",
    "        l.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321c84d-aa92-45e0-872e-008461dabe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyUnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a769c-7dcc-431a-9df2-3a2423c0c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_res = model.up[-1]           # End of the upsampling path\n",
    "zero_wgts(last_res.convs[-1][-1]) # Zero out the last convolution\n",
    "zero_wgts(last_res.idconv[0])     # Zero out the last ID connection\n",
    "zero_wgts(model.end.convs[-1][-1])# Zero out the weights for the end block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436535e-bb96-4138-9adb-7ee33a94f8b5",
   "metadata": {},
   "source": [
    "An alternative approach to the above cellblock is that instead of zeroing out the final blocks, we can multiply the weights and activations with a very small number such as `1e-3` to guarantee that there is no signal loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f369509-c475-49a2-b12f-09545b6f4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learner(model, dls, F.mse_loss, cbs=lr_cbs, opt_func=opt_func).lr_find(start_lr=1e-4, gamma=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f5409-d0cc-48c3-8669-445e48b785ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyUnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e655c-3c13-4167-b7d4-94eb8e356c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_res = model.up[-1]\n",
    "zero_wgts(last_res.convs[-1][-1])\n",
    "zero_wgts(last_res.idconv[0])\n",
    "zero_wgts(model.end.convs[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d83fd2-199a-4d80-88b1-4a53b83e6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, F.mse_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64474d-d634-49eb-8166-275c4bae849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54d035-84bf-42f4-9e96-e82a10ac64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(learn.model, 'models/superres-cross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7abaf-7c5e-4a1b-91b0-58800a8c5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, t, inp = learn.capture_preds(inps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556dcb2-3dd4-4162-9a70-f03fccc4728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "show_images(denorm(inp[:9]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b381abd-a058-4a93-8d84-74a2ffe5d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs\n",
    "show_images(denorm(p[:9]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bc413-3f02-4b02-a93f-8145669b7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training samples.\n",
    "show_images(denorm(t[:9]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62fa3f1-2fcf-4ea3-a293-161d77608c7c",
   "metadata": {},
   "source": [
    "Although we haven't recreated the originals, this approach's outputs are a marked improvement over the autoencoder approach.\n",
    "\n",
    "However, using MSE as a loss function for a model such as this is incorrect and results in blurry outputs like we saw above. This is a result of averaging the loss during the training steps.\n",
    "\n",
    "To fix this, **Perceptual Loss** is the recommended metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4057b7-7905-40d2-9a24-ec37e808935b",
   "metadata": {},
   "source": [
    "## **Perceptual Loss To Replace MSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47095010-eb0f-474e-8e6b-abe75cc055d6",
   "metadata": {},
   "source": [
    ">### **[What Are Perceptual Loss Functions?](https://deepai.org/machine-learning-glossary-and-terms/perceptual-loss-function)**\n",
    ">\n",
    ">**Perceptual loss** functions are designed to capture perceptual differences between images, such as content and style discrepancies, which are not always evident at the pixel level. They are often employed in tasks where the goal is to generate images that are visually pleasing to humans, such as in neural style transfer, super-resolution, and image synthesis.\n",
    ">\n",
    ">The core idea behind perceptual loss is to use the feature maps from various layers of a CNN, which has been pre-trained on a large dataset like ImageNet. By extracting these feature maps from both the target image and the generated image, we can compute the difference in the high-level features that the network has learned to detect, such as edges, textures, and patterns.\n",
    ">\n",
    ">### **How Perceptual Loss Functions Work**\n",
    ">\n",
    ">One common approach to implementing perceptual loss involves using a pre-trained VGG network, a type of CNN that has been shown to be effective in capturing image content and style. The perceptual loss function typically consists of two main components:\n",
    ">\n",
    ">    **Content Loss**: This measures how much the feature maps of the generated image differ from the feature maps of the target image. By minimizing this loss, the generated image is encouraged to preserve the content of the target image.\n",
    ">\n",
    ">    **Style Loss**: This measures the difference in the correlation between feature maps, capturing the texture and style information. Minimizing style loss ensures that the style of the generated image matches the style of a reference image.\n",
    ">\n",
    ">During the training process, the generated image is passed through the pre-trained network, and its feature maps are extracted at predetermined layers. The same is done for the target and style reference images. The perceptual loss is then calculated by comparing these feature maps using a distance metric, such as the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33edf0c-c057-40cf-8723-b3c846d594ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "cmodel = torch.load('models/imgnet-tiny-basic-25').cuda()# Train and fill in imgnet-tiny-custom model. For now\n",
    "                              # lets use the base tiny imgnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bfc6f-a745-47d7-aa17-efe2182394fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(dls.valid))\n",
    "with torch.autocast('cuda'), torch.no_grad(): # Automatic mixed precision\n",
    "    preds = to_cpu(cmodel(yb.cuda().half()))\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f1adc-869f-4afb-946d-45e977f7f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same code as before for the SynSets\n",
    "id2str = (path/'wnids.txt').read_text().splitlines()\n",
    "all_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\n",
    "synsets = {k:v.split(',', maxsplit=1)[0] for k, v in all_synsets if k in id2str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521ea18-30c4-4ff0-b182-7fe05da07072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles are off since we are working on model predictions and the model had ~60% accuracy\n",
    "titles = [synsets[id2str[o]] for o in preds.argmax(dim=1)]\n",
    "show_images(denorm(yb[:16]), imsize=2.5, titles=titles[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27312d49-9064-49d9-9da1-5e452c3518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5e879-4029-4775-a188-1fbc59a5c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the architecture above, we will grab the end of the 3rd Resblock\n",
    "for i in range(4, len(cmodel)): del(cmodel[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79fd85-9ef0-4460-aa03-4e79b97a58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast('cuda'), torch.no_grad():\n",
    "    feat = to_cpu(cmodel(yb.cuda())).float()\n",
    "    t = to_cpu(learn.model(yb.cuda())).float()\n",
    "    pred_feat = to_cpu(cmodel(t.cuda())).float()\n",
    "\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d367437-3e85-4f3d-91b3-bc82a5a00484",
   "metadata": {},
   "source": [
    "The intuitive approach is to ensure that the features in `cmodel` generally have the same sign as features in the `learn.model`.\n",
    "\n",
    "Moving onto the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f83aef-0f78-4693-8aa2-3109c23e18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate MSE Loss b/w inputs and targets, plus the MSE Loss\n",
    "# for the features we get out of the cmodel and the features of the target image.\n",
    "def comb_loss(inp, tgt):\n",
    "    with torch.autocast('cuda'):\n",
    "        # Target features must not get modified i.e. no_grad()\n",
    "        with torch.no_grad(): tgt_feat = cmodel(tgt).float() \n",
    "        # Inputs with have grad()    \n",
    "        inp_feat = cmodel(inp).float()\n",
    "    feat_loss = F.mse_loss(inp_feat, tgt_feat)\n",
    "    return F.mse_loss(inp, tgt) + feat_loss/10 # standardize feature loss for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184b97e-7409-4a4e-9a2a-85deb03f60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    model = TinyUnet()\n",
    "    last_res = model.up[-1]\n",
    "    zero_wgts(last_res.convs[-1][-1])\n",
    "    zero_wgts(last_res.idconv[0])\n",
    "    zero_wgts(model.end.convs[-1][-1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5e3bc-25f1-4026-9285-82419fcfbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learner(get_unet(), dls, comb_loss, cbs=lr_cbs, opt_func=opt_func).lr_find(start_lr=1e-4, gamma=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c43d45-92d6-4ea6-b00e-cf54faf9cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(get_unet(), dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af495a0-5186-4085-b278-938d84353eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b93e3-930e-4f7f-9973-66951c1d9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, t, inp = learn.capture_preds(inps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b690586-60ca-4b1a-bb2e-e532c69f40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(inp[:9]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83312561-4652-4c30-b18d-8c4553d39e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(p[:9]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f802e-d735-4267-8b55-ba3d40ab3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(t[:9]), imsize=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225632c7-f5b2-421b-856f-abe823001091",
   "metadata": {},
   "source": [
    "## **Unfreezing The Pretrained UNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b11788-6f1e-4bff-a702-e3b258b14c71",
   "metadata": {},
   "source": [
    "In this section, we will take the pretrained Tiny Imagenet model's weights and copy them over to the ResBlock at the start of the UNet model i.e. `model.start` - effectively fine-tuning the model.\n",
    "\n",
    "It it important to note that we will have random weights in the upsampling path and pre-trained weights in the downsampling path so setting `requires_grad_(False)` at appropriate stages is key.\n",
    "\n",
    "`Note` Execute the next few sections once the imgnet custom models have been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14888ff8-28f1-451a-8823-c6f560e06579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78858f98-21ef-4a5a-9c50-c1f902a7cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodel = torch.load('models/imgnet-tiny-basic-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ffbaf-eaea-4d8f-aeed-4a2763d5b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All nn.Module components have state_dict() which allows for weights to be\n",
    "# copied from one model to another.\n",
    "# Copying ResBlocks ==> pmodel[0] to model.start \n",
    "model.start.load_state_dict(pmodel[0].state_dict())\n",
    "# Since our model has 5 ResBlocks\n",
    "for i in range(5): model.dn[i].load_state_dict(pmodel[i+1].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038272a-1a40-4816-9eaf-fe08673ef747",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in model.dn.parameters(): o.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68595331-de2e-4021-b1e5-cd7ffdc41857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one epoch of the upsampling path to train the random weights before\n",
    "# moving onto the downsampling path.\n",
    "epochs = 1\n",
    "lr = 3e-3\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40116cb6-6a21-42b5-8001-e16b8b162ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b1d42-e218-4250-8b37-4db5f1ec593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in model.dn.parameters(): o.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bb021-76fa-444f-9317-b0b565ae4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 3e-3\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f26dfc-f7e7-4e4e-b109-f2774e0700ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d85b52-ccad-4702-a700-6386d996e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/superres-pcp.pkl')\n",
    "# learn.model = torch.load('models/superres-pcp.pkl').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7a136-4c3c-4ee0-987a-89663db6e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "p,t,inp = learn.capture_preds(inps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0536ee-18c0-4abc-ac37-0d5073a0db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(inp[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8cfdd-117d-457e-8882-1e2abeea80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(p[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9c95b-83f0-4780-a983-2135b175a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(t[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e57acf-8fd2-4828-8671-354e41db7c92",
   "metadata": {},
   "source": [
    "## **Cross Connections / Convolutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd535e0-60aa-4ccb-8db3-95c5f8226b00",
   "metadata": {},
   "source": [
    "The idea here is to introduce ResBlocks _within_ the cross connections of each downsampling and upsampling layer prior to concatenating or summing the outputs of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58026461-5a2b-46b6-aa33-db9b24d97dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_conv(nf, act, norm):\n",
    "    return nn.Sequential(\n",
    "        ResBlock(nf, nf, act=act, norm=norm),\n",
    "        nn.Conv2d(nf, nf, 3, padding=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbb855-8c79-4572-bffa-3fe461665f6c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rewriting TinyUnet class to incorporate cross connection convs.\n",
    "class TinyUnet(nn.Module):\n",
    "    def __init__(self, act=act_gr, nfs=(32, 64, 128, 256, 512, 1024), norm=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.start = ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)\n",
    "        self.dn = nn.ModuleList([ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "                                 for i in range(len(nfs)-1)])\n",
    "        # Cross connection conv. / resblock\n",
    "        self.xs = nn.ModuleList([cross_conv(nfs[i], act=act, norm=norm)\n",
    "                                 for i in range(len(nfs)-1, 0, -1)])\n",
    "        self.xs += [cross_conv(nfs[0], act=act, norm=norm)]\n",
    "        self.up = nn.ModuleList([upblock(nfs[i], nfs[i-1], act=act, norm=norm)\n",
    "                                 for i in range(len(nfs)-1, 0, -1)])\n",
    "        self.up += [ResBlock(nfs[0], 3, act=act, norm=norm)]\n",
    "        self.end = ResBlock(3, 3, act=nn.Identity, norm=norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = []\n",
    "        layers.append(x)\n",
    "        x = self.start(x)\n",
    "        for i, l in enumerate(self.dn):\n",
    "            layers.append(x)\n",
    "            x = l(x)\n",
    "        n = len(layers)\n",
    "        for i, l in enumerate(self.up):\n",
    "            if i!=0: x += self.xs[i](layers[n-i]) # Add the cross conv\n",
    "            x = l(x)\n",
    "        return self.end(x + layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d754069-1a8a-4d1e-a471-60af697a263f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmodel = torch.load('models/inettiny-custom-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5508bd-c2ef-42c9-a9bc-f7ccd159274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89458946-ea49-444e-8b81-1b03534358c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.start.load_state_dict(pmodel[0].state_dict())\n",
    "for i in range(5): model.dn[i].load_state_dict(pmodel[i+1].state_dict())\n",
    "for o in model.dn.parameters(): o.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f45ec-d537-4db2-94cc-68921f4d9e2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 3e-3\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4027d8-4221-4268-ba3f-372499985ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f70ae-8456-4895-8633-5dc21e106bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in model.dn.parameters(): o.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1fe62-c3e2-4570-b870-c017d28f7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epohcs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d0d36-c284-4ad2-a061-c1bd2aa09742",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150550a-dee9-4b2c-b30d-e1eb3917fb37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p,t,inp = learn.capture_preds(inps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe3c21-03c0-47a6-bf1a-3a2a2b9b56e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_images(denorm(inp[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111846f-f10e-4056-b6f9-20e7a755bcf4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_images(denorm(p[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9b0b7-8e48-4b75-a97e-12eea5626f1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_images(denorm(t[:9]), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220208d5-24f0-46ed-8888-68e60c683113",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/superres-cross.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
