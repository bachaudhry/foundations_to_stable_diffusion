{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e85b05b-04b3-4f24-bb1c-60d7322cca08",
   "metadata": {},
   "source": [
    "# **Experiment Tracking with Weights and Biases (W&B)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d4638-ed4f-47c2-bf10-a0a9d7629c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "import pickle,gzip,math,os,time,shutil,torch,random,logging\n",
    "import fastcore.all as fc\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *\n",
    "from miniai.accel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2806ff-7b9d-4256-ba21-d49772753a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d289794-ca8c-4b24-a931-6784494b5784",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a6794-a34d-423f-a5d5-a5e2a4bef6b4",
   "metadata": {},
   "source": [
    "For this notebook, we will be working with:\n",
    "- [CIFAR10](https://paperswithcode.com/dataset/cifar-10) dataset, which is a step up in complexity when compared to Fashion MNIST and is usually considered as the smallest dataset in alot of generative and CV research.\n",
    "- [Weights and Biases](https://wandb.ai) for experiment tracking.\n",
    "\n",
    "We're doing this to make sure that our ideas, which were used to create most of the MiniAi functionality, hold when new datasets, features and tools - such as W&B - are added to the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccca6c-bda3-4571-ae4d-5b6614c5ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl = 'img', 'label'\n",
    "name = \"cifar10\"\n",
    "dsd = load_dataset(name)\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [TF.to_tensor(o)-0.5 for o in b[xl]]\n",
    "\n",
    "bs = 32\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c4d83-37c0-4b32-8b57-e7832331852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data loader with xb and yb\n",
    "dt = dls.train\n",
    "xb, yb = next(iter(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3dcb8-c773-4a39-92ad-650628d37614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're now working with 3 channel i.e. RGB, images\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c17ca1-a3e4-4d0f-aba7-904f4e1c6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images in batch\n",
    "# Adding 0.5 fixes brightness problems\n",
    "show_images(xb[:25]+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe7db6-a08a-48e5-a7bd-cbfa04172690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def linear_sched(beta_min=0.0001, beta_max=0.02, n_steps=1000):\n",
    "    beta = torch.linspace(beta_min, beta_max, n_steps)\n",
    "    # SimpleNamespace allows us to initialize attributes while constructing the object.\n",
    "    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c85036-b05f-46a0-b503-13820fc8b1be",
   "metadata": {},
   "source": [
    "We will use the same `noisify()` function to add noise in channel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497e33f-c13f-401e-883d-781ef247b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "lin_abar = linear_sched(beta_max=0.01)\n",
    "alphabar = lin_abar.abar\n",
    "alpha = lin_abar.a\n",
    "sigma = lin_abar.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d7d9d-da63-4ea6-b281-4d0b3a1cfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisify(x0, ᾱ):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade185e5-4d26-4bef-a37e-8f21265a9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xt, t), ε = noisify(xb[:25], alphabar)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5e596-f78e-444b-8b03-7cf6410e53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing noisified images\n",
    "titles = fc.map_ex(t[:25], '{}')\n",
    "show_images(xt[:25].clip(-0.5, 0.5) + 0.5, imsize=1.5, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cef22-16d9-49eb-b959-59e2bc0db28e",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff405b6c-fda8-4c0d-8cf6-dc2d4d36cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the ResNet blocks from before\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x): return super().forward(*x).sample\n",
    "\n",
    "def init_ddpm(model):\n",
    "    for o in model.down_blocks:\n",
    "        for p in o.resnets:\n",
    "            p.conv2.weight.data.zero_()\n",
    "            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n",
    "\n",
    "    for o in model.up_blocks:\n",
    "        for p in o.resnets: p.conv2.weight.data.zero_()\n",
    "\n",
    "    model.conv_out.weight.data.zero_()\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\n",
    "\n",
    "def dl_ddpm(ds, nw=4): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9ce7b-c307-4ba1-b277-5e1eceac54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861c7da-7a32-4707-aeb6-4c841f6d600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model we created for FashionMNIST\n",
    "model = UNet(in_channels=3, out_channels=3, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6c1ea-db8c-4553-80a5-c4e6aa894ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving away from our previous model defaults to the ones used in Diffusers\n",
    "model = UNet(in_channels=3, out_channels=3)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb709c9f-c956-4a70-b424-406376328e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e1cc8-8ffd-4baa-9bd4-634d03bbe2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model for training, we will train for only one epoch\n",
    "# and then use the sampling technique below to get judge the model's initial\n",
    "# performance.\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n",
    "model = UNet(in_channels=3, out_channels=3)\n",
    "\n",
    "init_ddpm(model)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd174a8b-3f07-4ecf-904f-dcb582b41db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf9d6e-3fc1-46d9-acff-a4ca02dc5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also carry over the sampling function before before\n",
    "def sample(model, sz):\n",
    "    ps = next(model.parameters())\n",
    "    x_t = torch.randn(sz).to(ps)\n",
    "    preds = []\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=ps.device, dtype=torch.long)\n",
    "        z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(ps)\n",
    "        ᾱ_t1 = alphabar[t-1] if t > 0 else torch.tensor(1)\n",
    "        b̄_t = 1 - alphabar[t]\n",
    "        b̄_t1 = 1 - ᾱ_t1\n",
    "        noise = model((x_t, t_batch))\n",
    "        x_0_hat = ((x_t - b̄_t.sqrt() * noise)/alphabar[t].sqrt())\n",
    "        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n",
    "        preds.append(x_t.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f16901-8dc6-4377-a1f4-d71c7a7f2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = sample(model, (bs, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf87186-71a6-40a0-a325-7b6f265ee122",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (samples[-1] + 0.5).clamp(0, 1)\n",
    "show_images(s[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25592fed-ecce-4ec1-8fdc-de67b3af610d",
   "metadata": {},
   "source": [
    "While this approach can work, in reality, we will need to iterate quickly while carrying out both tracking and logging. Automating is usually the best way to go and one of those ways is to use Weights and Biases to handle all the tracking for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9828260-9b42-4ad4-8e15-c9e93a901f2c",
   "metadata": {},
   "source": [
    "## **Weights and Biases CB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795300bf-5ea0-4998-b6ba-a8f5cef400a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dc313-fee8-47c8-8a2f-bd29a497f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "class WandBCB(MetricsCB): # Inherting from MetricsCB\n",
    "    order = 100\n",
    "    def __init__(self, config, *ms, project='ddpm_cifar10', **metrics):\n",
    "        fc.store_attr()\n",
    "        super().__init__(*ms, **metrics)\n",
    "\n",
    "    def before_fit(self, learn): wandb.init(project=self.project, config=self.config)\n",
    "    def after_fit(self, learn): wandb.finish()\n",
    "\n",
    "    def _log(self, d): # Replacing the ProgessCB to allow W&B to handle logging\n",
    "        if self.train:\n",
    "            wandb.log({'train_'+m: float(d[m]) for m in self.all_metrics})\n",
    "        else:\n",
    "            wandb.log({'val_'+m: float(d[m]) for m in self.all_metrics})\n",
    "            wandb.log({'samples': self.sample_figure(learn)})\n",
    "        print(d)\n",
    "\n",
    "    def sample_figure(self, learn):\n",
    "        with torch.no_grad():\n",
    "            samples = sample(learn.model, (16, 3, 32, 32))\n",
    "        s = (samples[-1] + 0.5).clamp(0, 1)\n",
    "        plt.clf()\n",
    "        fig.axs = get_grid(16)\n",
    "        for im, ax in zip(s[:16], axs.flat): show_image(im, ax=ax)\n",
    "        return fig\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        super().after_batch(learn)\n",
    "        wandb.log({'loss': learn.loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87adce-bade-4e59-ad0f-3f61a268b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 2\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "wandbcb =  WandBCB(config={'lr':lr, 'epochs':epochs, 'comments':'default unet logging test'})\n",
    "cbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), wandbcb, BatchSchedCB(sched)]\n",
    "model = UNet(in_channels=3, out_channels=3)\n",
    "init_ddpm(model)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71712a40-601a-4dba-b64f-6cb2d069deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b870c-ebed-4404-96ef-00abd38ca23c",
   "metadata": {},
   "source": [
    "At present, logging using Weights and Biases takes a really long time, so I will dive deeper in the code to see if I can optimize further. \n",
    "\n",
    "Running checks in a cloud server is up next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d961710-91d1-4667-a527-dc88df015042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
