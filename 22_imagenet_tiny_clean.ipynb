{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2a24e4-de85-4450-b870-d28a6825c02f",
   "metadata": {},
   "source": [
    "# **Tiny ImageNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded86e2-4ee6-46a6-bb0d-7b4458a344bd",
   "metadata": {},
   "source": [
    "In this notebook we will be building on what has been developed so far, with respect to the Stable Diffusion pipeline. In particular, the focus will be on training a super resolution UNet using the `Tiny ImageNet` dataset. Note that `Fashion MNIST` won't be used here due to its 28x28 image resolution, whereas the ImageNet images are 64x64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9743f-9f6c-4ca9-a931-ed97b8117e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, timm, torch, random, datasets, math\n",
    "import fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import k_diffusion as K, torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF, torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from fastcore.foundation import L\n",
    "from torch import nn,tensor\n",
    "from operator import itemgetter\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from functools import partial\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "from glob import glob # unix style pathname pattern expansion\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *\n",
    "from miniai.accel import *\n",
    "from miniai.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d46f9-d6b5-46ee-9bcc-e0dd0d0337ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f645aa4-8325-43ee-9906-4168963897ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3802843-45fa-48f1-8240-2b1075009aac",
   "metadata": {},
   "source": [
    "## **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6a4e5-cac4-4d73-81fd-dafe2c8ab83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path = path_data/'tiny-imagenet-200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97dddb-61f8-4240-a775-bfc7eb864995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, tiny imagenet isn't widely available, but is still retained by Stanford.\n",
    "url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "if not path.exists():\n",
    "    path_zip = fc.urlsave(url, path_data)\n",
    "    shutil.unpack_archive('data/tiny-imagenet-200.zip', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf58ac1-cf67-4d40-9ddc-c9dec4db4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4e62e-b5ca-4fd9-b231-b789ef23b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple class to create a dataset \n",
    "class TinyDS:\n",
    "    def __init__(self, path):\n",
    "        self.path = Path(path)\n",
    "        # Pass specification of files to search\n",
    "        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, i): \n",
    "        # Return a tuple of items and the titles\n",
    "        return self.files[i], Path(self.files[i]).parent.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d67cd9-0860-44a4-85ba-a35e51ea6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = TinyDS(path/'train')\n",
    "tds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f208c-5a5f-4fe0-be8c-aab5bac784c7",
   "metadata": {},
   "source": [
    "Note that the directory is structured as follows: `train/*category code*/images/....JPEG`.\n",
    "\n",
    "The validation dataset isn't organized into subdirectories which match the training data. Specifically, there is a `val_annotations.txt` file which contains the necessary data for images and their categories. So additional data processing is required on this front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b95b3-28d3-4f09-b627-7086be10694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process val_annotations.txt file\n",
    "path_anno = path/'val'/'val_annotations.txt'\n",
    "# Create dict using a generator comprehension, split on tab and grab the first two cols\n",
    "anno = dict(o.split('\\t')[:2] for o in path_anno.read_text().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179361e-7694-4993-8536-43ddd209ab4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(path_anno.read_text()[:190])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c2dbf-5d99-4720-84d5-21f8cdc3a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inherited class to handle the validation data's different labelling approach.\n",
    "# Here the label won't be the parent.parent name - instead, it will be the name of the file.\n",
    "class TinyValDS(TinyDS):\n",
    "    def __getitem__(self, i): return self.files[i], anno[os.path.basename(self.files[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ffd27-a78b-4bec-b5c3-e6ed070a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = TinyValDS(path/'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80ded-aeed-4d57-bdbb-fc4094147d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds[0], len(tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f5594-4590-4e43-8701-470ff9416132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create a generic class which can transform any dataset. We can pass the \n",
    "# x or y i.e. independent and dependent variables. __init__() defaults to no-operation.\n",
    "class TfmDS:\n",
    "    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds, self.tfmx, self.tfmy = ds, tfmx, tfmy\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.ds[i]\n",
    "        return self.tfmx(x), self.tfmy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50c73f-9f88-40b5-9f14-0148c4c75ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset also provides a wordnet ID file, with 200 cat codes.\n",
    "# We will change the coding from its original form to an int.\n",
    "id2str = (path/'wnids.txt').read_text().splitlines()\n",
    "# Inverted enumeration\n",
    "str2id = {v:k for k, v in enumerate(id2str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbfc83-5648-4f7f-ae12-0f97b0997296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "dict(islice(str2id.items(), 0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958be25d-5e13-4357-84d3-abb26178368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying over xmean and xstd values of a batch for transformations\n",
    "xmean, xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499e2ff-a007-44bf-8342-7df1014c0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfmx(x):\n",
    "    img = read_image(x, mode=ImageReadMode.RGB) / 255\n",
    "    return (img - xmean[:, None, None])/xstd[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3febec9-01dc-4a58-8535-3f8c57924c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfmy(y): return tensor(str2id[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194573cf-f96b-4df3-9926-96915f0029f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_tds = TfmDS(tds, tfmx, tfmy)\n",
    "tfm_vds = TfmDS(vds, tfmx, tfmy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7e2a9-7643-4a8f-a472-4c80c9862a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi, yi = tfm_tds[2]\n",
    "id2str[yi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb686389-b9d8-419f-bf36-b3e539dc4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6dc7c-0f57-4596-8b56-7f44bfd95672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize image for viewing \n",
    "def denorm(x): return (x*xstd[:, None, None] + xmean[:, None, None]).clip(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b4f23-d5b6-4118-9c52-5795b6955629",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(denorm(xi));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a3870-9ca2-4ea2-b3f6-aee0905d22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for the training set.\n",
    "dltrn = DataLoader(tfm_tds, batch_size=bs, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7a398-85a0-4475-b3e8-b1558369d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = b = next(iter(dltrn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fac85-7f99-44cf-889f-834b83f870f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(denorm(xb[0]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40255249-5fc8-4765-83f0-367ab9469bb5",
   "metadata": {},
   "source": [
    "Now that we have enumerated the wordnet categories, we can use the additional `words.txt` file to grab titles for the keys for each image category. This is a large file with many additional codes, so the title grabbing will need to be selective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95677646-710f-4060-ae29-21db25357ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wordnet encoding schema is called `synsets`\n",
    "all_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\n",
    "all_synsets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750ddfa-701a-4a2c-98de-5ab9d2296080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to synsets which occur in our training data\n",
    "synsets = {k:v.split(',', maxsplit=1)[0] for k, v in all_synsets if k in id2str}\n",
    "# Extract titles\n",
    "titles = [synsets[id2str[o]] for o in yb]\n",
    "', '.join(titles[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab71f0-8194-413d-bf01-96d7968cebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(denorm(xb[:9]), titles=titles[:9], imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3cb1d-f7ed-4d79-b9e1-026b77c62273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformed and labelled data onto DataLoader\n",
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32a3ef-3282-402e-913e-5f9de6d7afa6",
   "metadata": {},
   "source": [
    "## **Train Basic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a9505-d687-483c-9457-eb26861ca6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_batch(b, tfm_x=fc.noop, tfm_y=fc.noop): return tfm_x(b[0]), tfm_y(b[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed3e70-634d-431a-9265-0fc0e750df2c",
   "metadata": {},
   "source": [
    "The standard procedure used for transforming images for super-resolution is `RandomResizeCrop`, but that tends to work poorly with 64x64 images and ends up introducing blurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a7770-8dfe-41f2-a93b-9213da4a8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n",
    "                     T.RandomHorizontalFlip(),\n",
    "                     RandErase())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ef14e-545c-4c5a-9c43-6f75082223fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "iw = partial(init_weights, leaky=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3b302-3b3c-4fb4-bc3d-ee5eae5a8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = (32, 64, 128, 256, 512, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812f501-b1a0-4a6b-9152-90f09ec5d702",
   "metadata": {},
   "source": [
    "Copying over the code we used to train the FashionMNIST UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c4359-7a38-4ecb-b7fa-44090356d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropmodel(act=act_gr, nfs=nfs, norm=nn.BatchNorm2d, drop=0.1):\n",
    "    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e2833-122c-4edd-8996-c7de74f897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TrainLearner(get_dropmodel(), dls, F.cross_entropy, cbs=[SingleBatchCB(), augcb, DeviceCB()])\n",
    "learn.fit(1)\n",
    "xb, yb = learn.batch\n",
    "show_images(denorm(xb.cpu())[:9], imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2890627-c50c-4c4b-a447-dd4d3e0c1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c766c9-3a2f-460f-9405-1432c3c794b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "lr_cbs = [DeviceCB(), augcb, MixedPrecision(), ProgressCB()]\n",
    "learn = Learner(get_dropmodel(), dls, F.cross_entropy, cbs=lr_cbs, opt_func=opt_func)\n",
    "# Find optimum LR for the UNet model\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fd5ea-faec-4762-9348-214915be61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca3d36-39d0-4f24-bf45-5f4b811d2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "lr = 0.01\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "learn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c0274-72cb-4743-93cb-50c9e6116e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca34ee2-c7dd-42cd-9439-17ca32978613",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/imgnet-tiny-basic-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1d47e-1b1b-412e-8b5d-e1ef8473f9d8",
   "metadata": {},
   "source": [
    "## **Training a Deeper Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a8363-e917-495a-aef2-03808c5f71c4",
   "metadata": {},
   "source": [
    "In the last training run, we managed to achieve an accuracy of `57.7%` on our standard ResNets with minimal data augmentation. To improve performance, we can refer to approaches used in [Papers With Code's](https://paperswithcode.com/sota/image-classification-on-tiny-imagenet-1) leaderboards.\n",
    "\n",
    "In addition to the standard architecture, we can add `resblocks` per downsampling layer to increase the depth of the ResNet to nearly 2x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4ef8f-5dc0-4a55-9672-ee9315d36347",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(learn)\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108d53b-7ffc-428a-a926-dd3a5e63623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add resblocks with additional depth. Simply goes through the predetermined\n",
    "# number of blocks and adds resblocks accordingly.\n",
    "def res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "    # For the last layer, use stride=2 else stride=1 for all other layers.\n",
    "    return nn.Sequential(*[\n",
    "        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n",
    "        for i in range(n_bk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6aaff-f205-42ca-adab-438da9db1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of resblocks for downsampling i.e. 9 resblocks in total\n",
    "# which is basically the sum of nbks\n",
    "nbks = (3, 2, 2, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e23c0-1113-4597-a035-83a59fb83e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n",
    "    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n",
    "    # Replace ResBlock() with res_blocks() for additional depth.\n",
    "    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f4104-41f1-4ecf-88b6-38f00dcf394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TrainLearner(get_dropmodel(), dls, F.cross_entropy, cbs=[SingleBatchCB(), augcb, DeviceCB()])\n",
    "learn.fit(1)\n",
    "xb, yb = learn.batch\n",
    "show_images(denorm(xb.cpu())[:9], imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b106cb-aca3-4689-9782-c0e329665c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d8f3b-12d9-42bc-9b26-02c2112778be",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(learn)\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef95e67-277b-4d31-bea5-851ac52c8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "learn = Learner(get_dropmodel(), dls, F.cross_entropy, cbs=lr_cbs, opt_func=opt_func)\n",
    "\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19436ca7-a22b-4d79-81aa-a561f10788d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n",
    "\n",
    "epochs = 20\n",
    "lr = 3e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "learn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffcdb8-1b78-4ae4-ad72-284f2713d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2ed1c-162a-4532-820a-47efe9b789cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/imgnet-tiny-custom-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff103ef-9752-491c-9612-286edd2e23c7",
   "metadata": {},
   "source": [
    "## **Additional Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f46192-3386-48af-9f55-ca49a1d6500b",
   "metadata": {},
   "source": [
    "Following updates to the model architecture, we will add additional augmentation. For this run, we will experiment with an approach called [TrivialAugment](https://arxiv.org/abs/2103.10158) which is:\n",
    "\n",
    "> a most simple baseline...that outperforms previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image.\n",
    "\n",
    "The paper states that it results in marked performance improvements to image modelling tasks. Techincal details of its application can be found in [Pytorch's documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280edaf-cadc-4b12-8d2a-7ccabd068278",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n",
    "                         T.RandomHorizontalFlip(),\n",
    "                         T.TrivialAugmentWide())\n",
    "\n",
    "norm_tfm = T.Normalize(xmean, xstd)\n",
    "erase_tfm = RandErase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465983cc-076a-4186-9966-49b5fdf1e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a1aef-758c-4639-858a-509b5078d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the augmentations in TrivialAugmentWide() require images to be in PIL format.\n",
    "# So we will both convert images to PIL objects, and carry out augmentations in one simple function.\n",
    "def tfmx(x, aug=False):\n",
    "    x = Image.open(x).convert('RGB')\n",
    "    if aug: x = aug_tfms(x)\n",
    "    x = TF.to_tensor(x)\n",
    "    x = norm_tfm(x)\n",
    "    # Random erase needs to happen after normalization due to handling of 0/1 gaussian noise.\n",
    "    if aug: x = erase_tfm(x[None])[0]\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f16e94-cc30-4917-a2eb-f722f1bb64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations only apply to the training set!\n",
    "tfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\n",
    "tfm_vds = TfmDS(vds, tfmx, tfmy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406d67f-ab31-4250-aaf6-35a37fcd9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a967b82-237e-423b-ac72-a54328638b6e",
   "metadata": {},
   "source": [
    "Based on the paper [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027), Kaiming He et al. proposed an updated ResNet which uses **\"pre-activation resblocks\"**.\n",
    "\n",
    "For this, we will need to redefine `conv()`, with dual activations in the `_conv_block()`. Activations within the `forward()` function of the `ResBlock()` will be removed.\n",
    "\n",
    "Finally, `get_dropmodel()` will have an additional activation and normalization i.e.\n",
    "\n",
    "```\n",
    "layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "```\n",
    "\n",
    "The details in these code blocks are highly important and warrant additional scrutiny before implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e6429-8952-4fc1-ae90-31aa1b5e6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True):\n",
    "    layers = []\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act:  layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n",
    "    return nn.Sequential(conv(ni, nf, stride=1    , act=act, norm=norm, ks=ks),\n",
    "                         conv(nf, nf, stride=stride, act=act, norm=norm, ks=ks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f07d2-133a-4ee1-9e9f-116a7bea38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resblock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "        super().__init__()\n",
    "        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n",
    "\n",
    "def get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n",
    "    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n",
    "    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b323e-9bcc-4728-a10c-714380067bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(): return get_dropmodel(nbks=(4,3,3,2,1), drop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88a131-d092-44ec-9a7f-7d42ca5a72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=[SingleBatchCB(), DeviceCB()])\n",
    "learn.fit(1)\n",
    "xb,yb = learn.batch\n",
    "show_images(denorm(xb.cpu())[:9], imsize=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f0aa2-79a3-40af-a13f-9828b82fa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 0.1\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = Learner(get_model(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d38fe-4f2b-419f-9e2c-b986918212c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49cc08-920b-43be-8019-d75849158e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/imgnet-tiny-trivaug-50')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
