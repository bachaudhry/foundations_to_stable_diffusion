{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f39363-6601-41c6-8e11-5f214ce26c41",
   "metadata": {},
   "source": [
    "# **Setting Up A Diffusion UNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2744b50-e0bc-45c8-b701-908f8eaf73c7",
   "metadata": {},
   "source": [
    "In this NB, we will train an unconditional diffusion model from scratch which will mostly be built using the pipeline components we've already built for this course - in addition to the model specific components from the **Karras Implementation NB i.e. 21_karras_implementation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625af93-a79c-4d6f-a622-65a23580d131",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fe828-c21c-475f-8331-b85d0e6324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, timm, torch, random, datasets, math, fastcore.all as fc\n",
    "import numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import k_diffusion as K, torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF, torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from fastcore.foundation import L\n",
    "from torch import nn, tensor\n",
    "from datasets import load_dataset\n",
    "from operator import itemgetter\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from functools import partial\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *\n",
    "from miniai.accel import *\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8 : fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56369d6-aa68-4812-addb-92895f9a2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar\n",
    "from diffusers import UNet2DModel, DDIMPipeline, DDPMPipeline, DDIMScheduler, DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90098a47-1cc4-491c-bf4a-a91aabf48d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "mpl.rcParams['figure.dpi'] = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b28fc-3344-4b0c-af41-8adf68d8b22c",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd713d-4fa3-42a8-b18f-19bdbf32431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl = 'image', 'label'\n",
    "name = \"fashion_mnist\"\n",
    "n_steps = 1000\n",
    "bs = 512\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b3828-05e2-402f-a72d-3346ab7f7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the standard deviation of the input data as sigma. Bear in mind that the inplace tfms\n",
    "# will have an impact on this value.\n",
    "sig_data = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6375c-e684-4012-b020-3b70d234574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2, 2, 2, 2))*2-1 for o in b[xl]]\n",
    "\n",
    "def scalings(sig):\n",
    "    # Total variance at a particular level of sigma\n",
    "    totvar = sig**2 + sig_data**2\n",
    "           #c_skip           , # c_out                   , #c_in\n",
    "    return sig_data**2/totvar, sig*sig_data/totvar.sqrt(), 1/totvar.sqrt()\n",
    "\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    # Log normal distribution of sigmas\n",
    "    sig = (torch.randn([len(x0)])*1.2-1.2).exp().to(x0).reshape(-1, 1, 1, 1)\n",
    "    noise = torch.randn_like(x0, device=device)\n",
    "    # Calculate values to pick an input between a clean image and pure noise\n",
    "    c_skip, c_out, c_in = scalings(sig)\n",
    "    noised_input = x0 + noise*sig\n",
    "    # The target is based on a mixture of both noise and clean images with scaling\n",
    "    # being done by c_out\n",
    "    target = (x0 - c_skip*noised_input) / c_out\n",
    "    # Noised input is scaled up or down using c_in\n",
    "    return (noised_input*c_in, sig.squeeze()), target\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl])\n",
    "def dl_ddpm(ds)    : return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0ee17-f414-4832-ae2e-0d037670522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d244907-184f-42a6-a1c6-ffd6b585fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409f0d7-5a37-4f9f-8295-6d1d3c85b53a",
   "metadata": {},
   "source": [
    "## **Train Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae42937-14ed-4a0c-905a-5692fd0bee0f",
   "metadata": {},
   "source": [
    "The unconditional model will be trained using the UNet architecture from previous NBs and the **Diffusers** library.\n",
    "\n",
    "Additionally, we will be using the [SiLU](https://mlarchive.com/machine-learning/activation-functions-all-you-need-to-know/) or the Sigmoid Activation Function.\n",
    "\n",
    "![title](imgs/SiLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44938d6-c8c4-493b-abba-3f18423f778e",
   "metadata": {},
   "source": [
    "Using the same convolution as the one from Tiny Imagenet, which is also called the **pre-activation convolution**. \n",
    "\n",
    "Preactivation convolution refers to a specific architectural design in NNs where the batch normalization and activation functions are applied before the convolution operation. This approach is primarily associated with enhancing the performance of deep learning models, particularly in residual networks (ResNets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e5564-dc54-40ef-b51b-e186943bbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d20a7-eed2-4d90-9999-61c72714b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure is same as previous ResNet blocks with the exception that there is no option\n",
    "# for down sampling and strides. That will be featured in the down_block(). This approach is similar\n",
    "# to the one used in Diffusers.\n",
    "class UnetResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.convs = nn.Sequential(unet_conv(ni, nf, ks, act=act, norm=norm),\n",
    "                                   unet_conv(nf, nf, ks, act=act, norm=norm))\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "\n",
    "    def forward(self, x): return self.convs(x) + self.idconv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea2acc-dee4-48a2-ab29-f95ef03267f7",
   "metadata": {},
   "source": [
    "By not adding _stride_ and _down-sampling_ to `UnetResBlock()`, we are ensuring that our approach is similar to the one used in the original `DDPM` architecture.\n",
    "\n",
    "We will try to simplify how different down-sampling blocks can be incorporated into UNets. One way to do this is to introduce the `SavedResBlock()` and `SavedConv()` modules. These two components have similar functionality as ResBlock() and Conv(), but are also able to store the activations. This makes the activations accessible as we develop the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8137ff-3e4a-4e03-b3a5-a4362ea5531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModule:\n",
    "    # Calls forward to grab the ResBlock and Conv results and stores them.\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        # Using Mixin which contains methods for use by other classes (multiple inheritance)\n",
    "        # without having to be the parent class of those other classes.\n",
    "        self.saved = super().forward(x, *args, **kwargs) \n",
    "        return self.saved\n",
    "\n",
    "# These classes only carry out Mixin ops for the target classes.\n",
    "class SavedResBlock(SaveModule, UnetResBlock): pass # multiple inheritance, First call is used with the second argument \n",
    "class SavedConv(SaveModule, nn.Conv2d): pass        # same as above. This allows UnetResBlock and Conv2d outputs to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306d4f8-9f94-45d3-9e37-9e394ca55176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(ni, nf, add_down=True, num_layers=1):\n",
    "    # SaveModule ops used Sequentially.\n",
    "    res = nn.Sequential(*[SavedResBlock(ni=ni if i==0 else nf, nf=nf)\n",
    "                         for i in range(num_layers)])\n",
    "    # Carry out down sampling if needed.\n",
    "    if add_down: res.append(SavedConv(nf, nf, 3, stride=2, padding=1))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8fa75-3ace-4513-a49a-2fdbe97cfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling will be done with a sequence of upsampling layers - followed by a simple 3x3 conv.\n",
    "# Again this approach is the one preferred by the Stable Diffusion team.\n",
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22060054-e096-4f4f-bc3f-21f43d2e545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    # Storing previous number of filters (activations and filters stored in prev_nf)\n",
    "    def __init__(self, ni, prev_nf, nf, add_up=True, num_layers=2):\n",
    "        super().__init__()\n",
    "        # Using the saved results in the upsampling path\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [UnetResBlock((prev_nf if i==0 else nf) + (ni if (i==num_layers-1) else nf), nf)\n",
    "             for i in range(num_layers)])\n",
    "        # Add an upsampling layer if asked.\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, ups):\n",
    "        # Call each resnet as we progress in the upsampling path. Concatenate downsampling activations with each upsampling\n",
    "        # layer at the end.\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1)) # Concatenate\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230e27e-b57f-4c09-aa84-033326fff106",
   "metadata": {},
   "source": [
    "The `UNet2dModel` class is pretty similar to our earlier approach. However, we will need to be able to store the activations from the downsampling blocks so that they may be used during upsampling. Also, there is the addition of a `self.mid_block` resblock in the downsampling path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ab45b-6326-4a9d-aa3b-0c4a1f4863a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DModel(nn.Module):\n",
    "    # Setting up input and output channels for RGB images. NFS are the same as the unconditional Stable Diffusion model.\n",
    "    def __init__(self, in_channels=3, out_channels=3, nfs=(224, 448, 672, 896), num_layers=1):\n",
    "        super().__init__()\n",
    "        # The create of a larger computational space, using nfs[0] is not particularly efficient.\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        nf = nfs[0]\n",
    "        self.downs = nn.Sequential()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            # The last down block doesn't have down sampling. Hence the condition.\n",
    "            self.downs.append(down_block(ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        # Adding an additional resblock during downsampling.\n",
    "        self.mid_block = UnetResBlock(nfs[-1])\n",
    "        # Setup the up-sampling path by reversal.\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        # Final convolution to turn 224 channels to 3 channels.\n",
    "        self.conv_out = unet_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Stores all the layes in `saved`\n",
    "        x = self.conv_in(inp[0])\n",
    "        saved = [x]\n",
    "        # Call sequential model and grab the saved activations.\n",
    "        x = self.downs(x)\n",
    "        saved += [p.saved for o in self.downs for p in o]\n",
    "        x = self.mid_block(x)\n",
    "        # pop activations from saved blocks.\n",
    "        for block in self.ups: x = block(x, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cf3ae-fe0e-4573-bf95-cc1c57a78b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the model since we are using fashion MNIST\n",
    "# For the upsampling block, there will be 3 layers since the UNet2DModel has -->\n",
    "# self.ups.append(UpBlock(ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "# This aligns with Diffusers and ends up saving the output of the downsampling.\n",
    "model = UNet2DModel(in_channels=1, out_channels=1, nfs=(32, 64, 128, 256), num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bd51a-8371-4f62-8ecc-b8fff43c537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752b31c-e2dd-4422-9f2d-4af8c34d6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs) # Pending migration to Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09becb-d838-445c-8981-4fbebd3631b3",
   "metadata": {},
   "source": [
    "This model leaves the **Time Embedding** and **Attention** modules. Lets work towards now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba2196-8c71-4821-b235-90ec0808db63",
   "metadata": {},
   "source": [
    "## **Building Upto A Time Step Model**\n",
    "\n",
    "We will work towards building a resblock with embeddings where `forward()` not only results in activations, but also in an additional parameter `t`. `t` is a vector which represents the embeddings of each time step aka temporal steps. These embeddings are similar to ones in NLP. Here each timestep will have its own vector representation. This idea applies to both discrete and continuous time steps _(Karras et al. used continuous sigmas)_. It should also be noted that two sigmas with similar values should have the same embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5a12d-e98d-481a-aeab-7c63d8c4c2de",
   "metadata": {},
   "source": [
    "### **A Note On (Sinusoidal) Time Steps / Time Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e69315-1c1a-44b5-b581-899d0ed47d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 16 # Embedding dimension size\n",
    "tsteps = torch.linspace(-10, 10, 100) # Timesteps though we won't have negative sigmas in practice.\n",
    "max_period = 10000 # Largest timestep. Also aligns with the max sequence length chosen by researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbf4ea-39a6-4563-a417-8a66a71b2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da2cbc6-1a48-4bcc-9e05-e2badb61a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exponent will have 8 embeddings instead of 16\n",
    "exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93318c8-c13c-435f-bf1a-9ed6c90fd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exponent);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0b5b5-6aea-461e-b428-8120e0966e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer product of exponent and time steps.\n",
    "emb = tsteps[:, None].float() * exponent.exp()[None, :]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6fa36-7307-4ff6-8d3e-77f460806517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(emb[0])\n",
    "plt.plot(emb[10])\n",
    "plt.plot(emb[20])\n",
    "plt.plot(emb[50])\n",
    "plt.plot(emb[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c5854-bd06-4013-97dc-c8691cccf99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sine waves across 100 sigmas\n",
    "emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef8ee8-d2ef-4936-b76b-eb6671862ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(emb[:, 0])\n",
    "plt.plot(emb[:, 1])\n",
    "plt.plot(emb[:, 2])\n",
    "plt.plot(emb[:, 3])\n",
    "plt.plot(emb[:, 4]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6b0e2-9ae7-4ed5-bd11-1b8845a543e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(emb[:, 8])\n",
    "plt.plot(emb[:, 9])\n",
    "plt.plot(emb[:, 10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408b745-5257-4b89-bbff-17c61f83e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenated embeddings represented across columns in the image below.\n",
    "show_image(emb.T, figsize=(8, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d632cf-2b5a-4c51-8606-a8b4c7fe56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formalizing into a single function.\n",
    "def timestep_embedding(tsteps, emb_dim, max_period=10000):\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:, None].float() * exponent.exp()[None, :]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0, 1, 0, 0)) if emb_dim%2==1 else emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50573401-f052-490c-acfb-a7e7e844c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(timestep_embedding(tsteps, 32, max_period=1000).T, figsize=(8, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d481a8-afab-44fe-bdd5-bc885905f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(timestep_embedding(tsteps, 32, max_period=1000).T, figsize=(8, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8fb03-ace3-4f8f-9d2f-bc1c38b7d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(timestep_embedding(tsteps, 32, max_period=10).T, figsize=(7,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0876c4-f289-4d2e-b665-403d710f8807",
   "metadata": {},
   "source": [
    "Based on the plots and how embedding spaces are utilized across different dimensions and max periods in particular, it becomes clear that the usage of `max_period=10000` is sub-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25888377-ce0d-4848-9818-2dfe58c603a4",
   "metadata": {},
   "source": [
    "### **Time Step Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ece87-ec7d-4644-be42-629b41d0981c",
   "metadata": {},
   "source": [
    "Lets build and train a UNet with the addition of timestep embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003bb06-97b3-4a33-8783-54562a0c0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a03294-50a5-4105-a3f4-4921ef844bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "??wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79221c-8077-4d0c-8046-08f4784e7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0152a7-4c90-4c14-8c5d-ee65508de35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Stable Diffusion team made their embedding dimension length twice as large as the \n",
    "# number of activations\n",
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        # Number of embeddings are projected to twice the number of filters\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = unet_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = unet_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        # Projects are passed through an activation function. Act is applied to HxW dims using broadcasting\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        # Chunk allows the extra large embedding dimension to be split into two variables.\n",
    "        scale, shift = torch.chunk(emb, 2, dim=1)\n",
    "        # One variable is added and the other is multiplied to the activations\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        return x + self.idconv(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b78bcf-55b3-4f32-b301-2c5fc238da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saved(m, blk):\n",
    "    # Takes a callable as input (for e.g. functions/modules etc.) and returns a callable which is identical\n",
    "    # to the input but is also saved in a list \n",
    "    m_ = m.forward\n",
    "    # Call original forward, call args and kwargs and then save the output\n",
    "    @wraps(m.forward)\n",
    "    def _f(*args, **kwargs):\n",
    "        res = m_(*args, **kwargs)\n",
    "        blk.saved.append(res)\n",
    "        return res\n",
    "\n",
    "    m.forward = _f # Replace modules forward function with _f\n",
    "    return m # Return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c514b-1bb8-4b8f-a257-99449376ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1):\n",
    "        super().__init__()\n",
    "        # Downsampling and resnet blocks contain saved versions of modules.\n",
    "        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf), self)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # We won't be using sequentials since timesteps are being passed to the resnets\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: x = resnet(x, t)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ab51f-9711-48e7-be91-dd1a0cddf363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf) + (ni if (i==num_layers-1) else nf), nf)\n",
    "             for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece8d21-ab77-427f-8306-efb2535d88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbUNetModel(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, nfs=(224, 448, 672, 896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        self.conv_out = unet_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x, t = inp # Tuple passed containing activations and the timesteps / sigmas\n",
    "        temb = timestep_embedding(t, self.n_temb) # Number of timestep embeddings set equal to the number of filters in self.conv_in\n",
    "        emb = self.emb_mlp(temb) # Take the t embeddings and return the actual embeddings to pass in the resnet blocks\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb) # Call downsampling and pass embeddings each time\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf399a-56a4-4b9c-a705-75f6a79643f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32, 64, 128, 256), num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b46c5-2ee2-46c2-af1f-a15c3ee7d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffef418-67f6-41b1-894d-3945f187d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673221e0-d812-4a54-8258-0067b3412d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6ac756-10b8-46a3-aaae-f6ae47f0d7f7",
   "metadata": {},
   "source": [
    "## **Sampling Using The Karras et al. Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f410a3-23a9-4428-ba68-fd126e95328b",
   "metadata": {},
   "source": [
    "Here, we will be copying over the sampling code from `21_karras_implementation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104de2fc-556b-4a34-bcdc-0a79d5be5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miniai.fid import ImageEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5914e8-9cf1-4d82-b675-4ba0ac71f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel = torch.load('models/data_aug2.pkl')\n",
    "del(cmodel[8])\n",
    "del(cmodel[7])\n",
    "\n",
    "bs = 2048\n",
    "tds2 = dsd.with_transform(transformi)\n",
    "dls2 = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n",
    "\n",
    "dt = dls2.train\n",
    "xb, yb = next(iter(dt))\n",
    "\n",
    "ie = ImageEval(cmodel, dls2, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123b8e0-1e24-411d-9ad3-ccae322d6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (512,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dc7ce-ab1b-4a59-abdc-8da6157b9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (2048,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b3595-1d66-4f22-840a-ba9cf09da963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmas_karras(n, sigma_min=0.01, sigma_max=80., rho=7.):\n",
    "    ramp = torch.linspace(0, 1, n)\n",
    "    min_inv_rho = sigma_min**(1/rho)\n",
    "    max_inv_rho = sigma_max**(1/rho)\n",
    "    sigmas = (max_inv_rho + ramp * (min_inv_rho-max_inv_rho))**rho\n",
    "    return torch.cat([sigmas, tensor([0.])]).cuda()\n",
    "\n",
    "def denoise(model, x, sig):\n",
    "    sig = sig[None] #* torch.ones((len(x),1), device=x.device)\n",
    "    c_skip,c_out,c_in = scalings(sig)\n",
    "    return model((x*c_in, sig))*c_out + x*c_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a44aa-8174-4f47-b81b-e8a715ad0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ancestral_step(sigma_from, sigma_to, eta=1.):\n",
    "    if not eta: return sigma_to, 0.\n",
    "    var_to,var_from = sigma_to**2,sigma_from**2\n",
    "    sigma_up = min(sigma_to, eta * (var_to * (var_from-var_to)/var_from)**0.5)\n",
    "    return (var_to-sigma_up**2)**0.5, sigma_up\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_euler_ancestral(x, sigs, i, model, eta=1.):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    denoised = denoise(model, x, sig)\n",
    "    sigma_down,sigma_up = get_ancestral_step(sig, sig2, eta=eta)\n",
    "    x = x + (x-denoised)/sig*(sigma_down-sig)\n",
    "    return x + torch.randn_like(x)*sigma_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204fbb1-6f22-45c0-95a0-0115d3924081",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_euler(x, sigs, i, model):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    denoised = denoise(model, x, sig)\n",
    "    return x + (x-denoised)/sig*(sig2-sig)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_heun(x, sigs, i, model, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    n = len(sigs)\n",
    "    gamma = min(s_churn/(n-1), 2**0.5-1) if s_tmin<=sig<=s_tmax else 0.\n",
    "    eps = torch.randn_like(x) * s_noise\n",
    "    sigma_hat = sig * (gamma+1)\n",
    "    if gamma > 0: x = x + eps * (sigma_hat**2-sig**2)**0.5\n",
    "    denoised = denoise(model, x, sig)\n",
    "    d = (x-denoised)/sig\n",
    "    dt = sig2-sigma_hat\n",
    "    x_2 = x + d*dt\n",
    "    if sig2==0: return x_2\n",
    "    denoised_2 = denoise(model, x_2, sig2)\n",
    "    d_2 = (x_2-denoised_2)/sig2\n",
    "    d_prime = (d+d_2)/2\n",
    "    return x + d_prime*dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61593620-0c8c-4a30-b2ea-275b49a19efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(sampler, model, steps=100, sigma_max=80., **kwargs):\n",
    "    preds = []\n",
    "    x = torch.randn(sz).cuda()*sigma_max\n",
    "    sigs = sigmas_karras(steps, sigma_max=sigma_max)\n",
    "    for i in progress_bar(range(len(sigs)-1)):\n",
    "        x = sampler(x, sigs, i, model, **kwargs)\n",
    "        preds.append(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea3e0d-bfb3-4add-b084-af7329965f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample_lms(model, steps=20, order=3)\n",
    "# preds = sample(sample_euler_ancestral, model, steps=100, eta=1.)\n",
    "# preds = sample(sample_euler, model, steps=100)\n",
    "# preds = sample(sample_heun, model, steps=20, s_churn=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c02ff7-86fe-459f-804f-95937c2bb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = preds[-1]\n",
    "s.min(),s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dd57c-6c40-420c-9149-ced036b7e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(s[:25].clamp(-1,1), imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9b19d-68a8-45ef-8ee9-120e07bbd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lms 20\n",
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95119b61-c3d9-4428-b797-374c5d51c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample_lms(model, steps=20, order=3)\n",
    "s = preds[-1]\n",
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ed05e-4ad0-45db-ac11-d02f324efdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample_lms(model, steps=20, order=3)\n",
    "s = preds[-1]\n",
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c6ea8-1faa-4a97-94b0-98a9047dfa95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa3133-caf5-4ebc-80e4-446061b1bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80bf0b-50b1-4294-b754-995cfb3ab018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_multistep_coeff(order, t, i, j):\n",
    "    if order-1 > i: raise ValueError(f'Order {order} too high for step {i}')\n",
    "\n",
    "    def fn(tau):\n",
    "        prod = 1.\n",
    "        for k in range(order):\n",
    "            if j == k: continue\n",
    "            prod *= (tau - t[i-k])/ (t[i-j] - t[i-k])\n",
    "        return prod\n",
    "    return integrate.quad(fn, t[i], t[i+1], epsrel=1e-4)[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_lms(model, steps=100, order=4, sigma_max=80.):\n",
    "    preds = []\n",
    "    x = torch.randn(sz).cuda() * sigma_max\n",
    "    sigs = sigmas_karras(steps, sigma_max=sigma_max)\n",
    "    ds = []\n",
    "    for i in progress_bar(range(len(sigs)-1)):\n",
    "        sig = sigs[i]\n",
    "        denoised = denoise(moodel, x, sig)\n",
    "        d = (x - denoised)/sig\n",
    "        ds.append(d)\n",
    "        if len(ds) > order: ds.pop(0)\n",
    "        cur_order = min(i+1, order)\n",
    "        coeffs = [linear_multistep_coeff(cur_order, sigs, i, j) for j in range(cur_order)]\n",
    "        x = x + sum(coeff*d for coeff, d in zip(coeffs, reversed(ds)))\n",
    "        preds.append(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000c00b-d33c-44df-8ffc-989ba7425687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
