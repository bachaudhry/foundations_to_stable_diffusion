{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41735e3-4044-4419-8e06-5a7b2260f2ff",
   "metadata": {},
   "source": [
    "# **Experiment with Training on a Pre-Trained Imagenet Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c481e-539d-4fad-bdf6-632aa4bd4ecd",
   "metadata": {},
   "source": [
    "One interesting exercise will be to apply Stable Diffusion using a pretrained model. Jeremy demostrated the application of ImageNet for this task. Recall that during the super resolution segment of the course, there was a huge difference in the performance of the pre-trained model compared to the newly trained model - with the prior clearly showing improved outputs compared to the latter. \n",
    "\n",
    "The same thinking can be applied here. However, we will need a pretrained latents model where the downsampling layers are pretrained on latents. A full ImageNet model, pretrained on latents should be up to the task.\n",
    "\n",
    "We can grab the full Imagenet model from Kaggle under the [ImageNet Object Localization Challenge](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f65b8-9525-4db0-a585-82290ebc7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "os.environ['OMP_NUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258ab65-2fc8-4149-b1cf-280d20c92920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip\n",
    "\n",
    "from glob import glob\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from fastprogress import progress_bar\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from miniai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53f63d-b435-4077-b37b-8fb45bb2354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324b4df-432a-472b-a76c-0895de33e0bb",
   "metadata": {},
   "source": [
    "We will download images from Kaggle to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4609e69-b69e-4127-aa85-408f13adadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')/'ILSVRC'\n",
    "path = path_data/'Data'/'CLS-LOC'\n",
    "\n",
    "dest = path_data/'latents' # Sub directory for latents\n",
    "dest.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d42fd3-b4b3-45ce-bcd7-75de238366ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fadd8-39ea-4e52-b428-b243096ccd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDS:\n",
    "    def __init__(self, path, spec):\n",
    "        # Creating a cached list of files will speed up the read-write process.\n",
    "        cache = path/'files.zpkl'\n",
    "        if cache.exists():\n",
    "            with gzip.open(cache) as f: self.files = pickle.load(f)\n",
    "        else:\n",
    "            self.files = glob(str(path/spec), recursive=True) # glob with different storage systems can take varying amounts of time.\n",
    "            with gzip.open(cache, 'wb', compresslevel=1) as f: pickle.dump(self.files, f) # convert with robust compression.\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        f = self.files[i]\n",
    "        im = read_image(f, mode=ImageReadMode.RGB) / 255\n",
    "        im = TF.resize(TF.center_crop(im, min(im.shape[1:])), 256) # Lazy center cropping and resizing\n",
    "        return im, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed02bc-2a09-450f-b604-bac8f4bb01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDS(path, '**/*.JPEG')\n",
    "dl = DataLoader(ds, batch_size=64, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a4b21-0df3-464b-8d6e-b4b2b9fa69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dl))\n",
    "xe = vae.encode(xb.cuda())\n",
    "xs = xe.latent_dist.mean\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09da59-0b6c-490a-9ea6-bc5ef27a7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(((xs[:16,:3])/4).sigmoid(), imsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ee57c-9fa3-47f8-9f76-3462495b98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = to_cpu(vae.decode(xs))\n",
    "show_images(xd['sample'][:16].clamp(0,1), imsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee49ae8-cf47-42f4-8b8d-fa972725f253",
   "metadata": {},
   "source": [
    "Documentation on [Object oriented file system paths in `pathlib`](https://docs.python.org/3/library/pathlib.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574c167-9e01-48bb-bd20-f2a3610ee43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create destination directory\n",
    "if not dest.exists():\n",
    "    dest.mkdir()\n",
    "    for xb, yb in progress_bar(dl):\n",
    "        eb = to_cpu(vae.encode(xb.cuda()).latent_dist.mean).numpy() # separate numpy file for each item\n",
    "        for ebi, ybi in zip(eb, yb): # Go through each item of the batch and save to relative directory\n",
    "            ybi = dest/Path(ybi).relative_to(path).with_suffix('')\n",
    "            (ybi.parent).mkdir(parents=True, exist_ok=True)\n",
    "            np.save(ybi, ebi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ab2f2-90e1-4aed-b479-00eff3cdae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDS(ImageDS): # Alternate approach to handling Numpy datasets.\n",
    "    def __getitem__(self, i):\n",
    "        f = self.files[i]\n",
    "        im = np.load(f)\n",
    "        return im, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599c562-ebb1-4325-8294-1976a0c76baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a599121-c39a-4f7c-a5e4-3e6c22a7192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = NumpyDS(dest/'train', '**/*.npy')\n",
    "vds = NumpyDS(dest/'val', '**/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e54f7-850b-4958-8c15-5f7ef3c416a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = DataLoader(tds, batch_size=bs, num_workers=0) # Training dataloader\n",
    "xb,yb = next(iter(tdl))\n",
    "# Mean and standard deviation on the channel dimension\n",
    "xb.mean((0,2,3)), xb.std((0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99171fea-da52-40b0-a77f-6fddf0b8f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store mean and standard deviation.\n",
    "xmean, xstd = (tensor([5.37007, 2.65468, 0.44876, -2.39154]),\n",
    "               tensor([3.99512, 4.44317, 3.21629, 3.10339]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e4ef1-de2d-4eb0-b5da-d0f84d525e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfmDS:\n",
    "    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds, self.tfmx, self.tfmy = ds, tfmx, tfmy\n",
    "\n",
    "    def __len__(self): return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.ds[i]\n",
    "        return self.tfmx(x), self.tfmy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5043e-b27c-4613-b38d-ca329e6143d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2str = (path_data/'imagenet_lsvrc_2015_synsets.txt').read_text().splitlines()\n",
    "str2id = {v:k for k,v in enumerate(id2str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a0529-d997-48a3-b591-1490aadeca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_tfms = nn.Sequential(T.Pad(2), T.RandomCrop(32), RandErase()) # augmentation with padding, random cropping and erasing\n",
    "norm_tfm = T.Normalize(xmean, xstd) # torchvision normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fb290-d5b2-4e12-b7c7-6dff1ad98a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfmx(x, aug=False):\n",
    "    x = norm_tfm(tensor(x))\n",
    "    if aug: x = aug_tfms(x[None])[0] # add unit axis to ensure a batch of one if augmentations are being applied.\n",
    "    return x\n",
    "\n",
    "def tfmy(y): return tensor(str2id[Path(y).parent.name]) # convert path names to ids.\n",
    "\n",
    "tfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\n",
    "tfm_vds = TfmDS(vds, tfmx, tfmy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563b10c-fd3e-4bcb-8500-d0d82c47cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x): return (x*xstd[:, None, None] + xmean[:, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d45f6-d431-4602-9815-5cbf182e0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a1060-6c16-4691-9486-7033cc4bcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = [o.split('\\t') for o in (path_data/'words.txt').read_text().splitlines()]\n",
    "synsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba11c18-8301-4b91-b8a4-14593c024f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dls.train))\n",
    "titles = [synsets[id2str[o]] for o in yb]\n",
    "xb.mean(),xb.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b2b87-dcb5-473d-9eb0-92c5f6e7d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = to_cpu(vae.decode(denorm(xb[:9]).cuda()))\n",
    "show_images(xd['sample'].clamp(0, 1), imsize=4, titles=titles[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada92b3-eb1d-4ac1-96c5-a65c348e0dce",
   "metadata": {},
   "source": [
    "## **Create and Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67911a5d-6d85-43c2-8e6b-893cd4e51c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "iw = partial(init_weights, leaky=0.1)\n",
    "\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b9a3f-8550-4762-89f9-6778046e4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True): # Diffusion backbone.\n",
    "    layers = []\n",
    "    # Preactivations works best for UNets\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n",
    "    return nn.Sequential(conv(ni, nf, stride=1, act=act, norm=norm, ks=ks),\n",
    "                         conv(nf, nf, sride=stride, act=act, norm=norm, ks=ks))\n",
    "\n",
    "def ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "        super().__init__()\n",
    "        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n",
    "\n",
    "def res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "    return nn.Sequential(*[\n",
    "        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n",
    "        for i in range(n_bk)\n",
    "    ])\n",
    "\n",
    "def get_dropmodel(nfs, nbks, act=act_gr, norm=nn.BatchNorm2d, drop=0.2): # Model with dropouts\n",
    "    layers = [nn.Conv2d(4, nfs[0], 5, padding=2)]\n",
    "    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 1000, bias=False), nn.BatchNorm1d(1000)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5a989-622c-4554-a70c-eefe98a9ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "# start off with fewer channels in earlier blocks to shift the burden of learning to later layers. Randomly dropping blocks at the\n",
    "# last layer as well.\n",
    "model = get_dropmodel(nbks=(1,2,4,3), nfs=(32, 64, 128, 512, 1024), drop=0.1)\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbf182-44f8-42b6-995b-1a8c898314be",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b0135-e809-4e5b-bd0d-ac55d793588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/imgnet-latents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5cf1e6-0651-412b-8259-ff424187fff4",
   "metadata": {},
   "source": [
    "Push modelling further by experimenting with:\n",
    "- New datasets on HF.\n",
    "- Carrying over architecture choices from superres. (resnets on cross connects etc.)\n",
    "- Try perceptual loss (do ensure the model is pretrained with latents!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcccc4-deb8-4952-b475-46c90ddb0e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f5a33-3e42-4c4a-b92d-6351921e32e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
